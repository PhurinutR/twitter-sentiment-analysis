{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65121d22",
   "metadata": {},
   "source": [
    "# TF-IDF with Naive Bayes Model (Document-Based)\n",
    "### Implementation: Scikit's TF-IDF and **PySpark-based** Naive Bayes models\n",
    "Author: Marcus KWAN TH\n",
    "\n",
    "Last updated: 2025-11-17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70cb89a",
   "metadata": {},
   "source": [
    "## 1. Prerequisite: Data extraction from `util`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f303eab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The purpose of this lines is to:\n",
    "1. Prevent PySpark from using a different Python interpreter\n",
    "2. Adding the root path to the sys context for the runtime to properly import util.preprocessing, otherwise, error will occur.\n",
    "\n",
    "Based on the testing with Jupyter Notebook.\n",
    "\"\"\"\n",
    "\n",
    "import sys, os\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# Add the root folder to sys.path before importing util package\n",
    "root_path = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if root_path not in sys.path:\n",
    "    sys.path.append(root_path)\n",
    "\n",
    "# Import all necessary library for TF-IDF embedding\n",
    "from pyspark.sql import SparkSession\n",
    "from util.preprocessing import load_and_preprocess_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bed50ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/17 14:06:11 WARN Utils: Your hostname, Marcuss-MacBook-Air.local, resolves to a loopback address: 127.0.0.1; using 10.11.97.189 instead (on interface en0)\n",
      "25/11/17 14:06:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/17 14:06:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/17 14:06:12 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/11/17 14:06:12 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Before running the following code, please ensure to zip the util folder using the command: \n",
    "1. cd (to the root directory)\n",
    "2. zip -r util.zip util\n",
    "\n",
    "This will create a util.zip file in the root directory.\n",
    "This step is necessary for PySpark to access the util package during distributed processing.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize Spark Session\n",
    "ss  = SparkSession.builder \\\n",
    "        .appName(\"Marcus TF-IDF Naive-Bayes Model\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Add util.zip to PySpark context \n",
    "spark = ss.sparkContext.addPyFile(\"../util.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c69066cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables:\n",
    "col_name = \"tf-idf vectors\"\n",
    "col_label = \"label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07eac53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/17 14:06:13 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load and preprocess training and testing data from the util package\n",
    "# Then convert to Pandas DataFrame for TF-IDF vectorization\n",
    "train_df = load_and_preprocess_data('../Twitter_data/traindata7.csv').toPandas()\n",
    "test_df = load_and_preprocess_data('../Twitter_data/testdata7.csv').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b19f3465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 596\n",
      "Number of testing samples: 397\n",
      "\n",
      "Training 1: wishin i could go to home depot to buy shit to build shit\n",
      "Testing 2: cold war black ops zombie be a damn hype!!!!!!\n"
     ]
    }
   ],
   "source": [
    "# Extract documents and labels from training data\n",
    "train_documents = train_df.iloc[:, 0].astype(str).tolist()\n",
    "train_labels = train_df.iloc[:, 1].tolist()\n",
    "\n",
    "# Extract documents and labels from testing data\n",
    "test_documents = test_df.iloc[:, 0].astype(str).tolist()\n",
    "test_labels = test_df.iloc[:, 1].tolist()\n",
    "\n",
    "print(f\"Number of training samples: {len(train_documents)}\")\n",
    "print(f\"Number of testing samples: {len(test_documents)}\\n\")\n",
    "\n",
    "print(f\"Training 1: {train_documents[0]}\")\n",
    "print(f\"Testing 2: {test_documents[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044a87be",
   "metadata": {},
   "source": [
    "## 2. TF-IDF Embedding Implementation\n",
    "### Vectorize the pre-processed data by documents.\n",
    "Libraries: Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6806b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary library for document-vectorization\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2abcd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply TF-IDF Vectorizer fit on training data\n",
    "vectorizer = TfidfVectorizer(min_df=4, max_df=0.95, ngram_range=(1,2), use_idf=True)\n",
    "train_tfidf_matrix = vectorizer.fit_transform(train_documents)\n",
    "\n",
    "# Apply TF-IDF Vectorizer on testing data\n",
    "test_tfidf_matrix = vectorizer.transform(test_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7473406",
   "metadata": {},
   "source": [
    "#### In a nut-shell,\n",
    "\n",
    "- **`fit()`**: Fit the vectorizer/model to the **training data** and save the vectorizer/model to a variable (returns sklearn.feature_extraction.text.TfidfVectorizer)\n",
    "\n",
    "- **`transform()`**: Use the variable output from `fit()` to transformer **validation/test data** (returns scipy.sparse.csr.csr_matrix)\n",
    "\n",
    "- **`fit_transform()`**: Used to directly transform the **training data**, essentially a combination of `fit()` + `transform()`, thus `fit_transform()`. (returns scipy.sparse.csr.csr_matrix)\n",
    "\n",
    "Source: https://stackoverflow.com/questions/53027864/what-is-the-difference-between-tfidfvectorizer-fit-transfrom-and-tfidf-transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4570386",
   "metadata": {},
   "source": [
    "## 3. Save the TD-IDF-vectorized documents to dataframes.\n",
    "Libraries: Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89e54e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape for (1) Train: (596, 673), (2) Test: (397, 673)\n",
      "Dimension size: 673\n"
     ]
    }
   ],
   "source": [
    "# Convert sparse (doc-term) matrices to dense arrays\n",
    "train_tfidf_dense = train_tfidf_matrix.toarray()\n",
    "test_tfidf_dense = test_tfidf_matrix.toarray()\n",
    "\n",
    "# Check the shape of the resulting TF-IDF matrices\n",
    "print(f\"TF-IDF matrix shape for (1) Train: {train_tfidf_dense.shape}, (2) Test: {test_tfidf_dense.shape}\")\n",
    "print(f\"Dimension size: {len(vectorizer.get_feature_names_out())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "211cbc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PySpark DataFrames for training and testing data for later stages\n",
    "train_spark_df = ss.createDataFrame(\n",
    "    [(Vectors.dense(vec), int(lbl)) for vec, lbl in zip(train_tfidf_dense, train_labels)],\n",
    "    [col_name, col_label]\n",
    ")\n",
    "\n",
    "test_spark_df = ss.createDataFrame(\n",
    "    [(Vectors.dense(vec), int(lbl)) for vec, lbl in zip(test_tfidf_dense, test_labels)],\n",
    "    [col_name, col_label]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12d36384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample training data:\n",
      "+--------------------------------------------------+-----+\n",
      "|                                    tf-idf vectors|label|\n",
      "+--------------------------------------------------+-----+\n",
      "|[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0....|    0|\n",
      "|[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0....|    0|\n",
      "|[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0....|    0|\n",
      "|[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0....|    0|\n",
      "|[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0....|    0|\n",
      "+--------------------------------------------------+-----+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample training data:\")\n",
    "train_spark_df.show(5, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9036821",
   "metadata": {},
   "source": [
    "## 4. Naive Bayes Data Analytic Model Implementation\n",
    "### Sentiment classification using Naive Bayes model using PySpark.\n",
    "Libraries: Pyspark (Naive Bayes classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19b2b605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for Naive Bayes classification\n",
    "from pyspark.ml.classification import NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6dedde4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Naive Bayes model on training data\n",
    "nb = NaiveBayes(featuresCol=col_name, labelCol=col_label, modelType=\"multinomial\", smoothing=1.0)\n",
    "model = nb.fit(train_spark_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce83e5df",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "- **Cannot** use `bernoulli` for the modelType of NaiveBayes as it is only suitable for binary classification.\n",
    "- Smoothing (range: [0.0, 1.0]) seems doesn't improve the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244607a0",
   "metadata": {},
   "source": [
    "## 5. Simple Evaluation\n",
    "### Examine the performance of the TF-IDF + Naive Bayes combination with training and testing loss with accuracy.\n",
    "Libraries: PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1f97b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for evaluation\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "878d4e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract probability of the true class for each test sample\n",
    "def get_prob(probability, label):\n",
    "    return float(probability[int(label)])\n",
    "get_prob_udf = udf(get_prob, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "091357ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training predictions\n",
    "train_predictions = model.transform(train_spark_df)\n",
    "train_predictions = train_predictions.withColumn(\n",
    "    \"true_prob\", get_prob_udf(col(\"probability\"), col(col_label))\n",
    ")\n",
    "\n",
    "# Testing predictions\n",
    "test_predictions = model.transform(test_spark_df)\n",
    "test_predictions = test_predictions.withColumn(\n",
    "    \"true_prob\", get_prob_udf(col(\"probability\"), col(col_label))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7903a5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/17 14:06:18 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.7550, Train log loss: 0.9433\n",
      "Test  accuracy: 0.5063, Test  log loss: 1.2211\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Accuracy and loss Evaluation\n",
    "\"\"\"\n",
    "# Evaluators of the accuracy and loss using PySpark\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=col_label, predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "loss_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=col_label, predictionCol=\"prediction\", metricName=\"logLoss\"\n",
    ")\n",
    "\n",
    "train_loss = loss_evaluator.evaluate(train_predictions)\n",
    "test_loss = loss_evaluator.evaluate(test_predictions)\n",
    "train_accuracy = accuracy_evaluator.evaluate(train_predictions)\n",
    "test_accuracy = accuracy_evaluator.evaluate(test_predictions)\n",
    "\n",
    "# Result\n",
    "print(f\"Train accuracy: {train_accuracy:.4f}, Train log loss: {train_loss:.4f}\")\n",
    "print(f\"Test  accuracy: {test_accuracy:.4f}, Test  log loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c424d36",
   "metadata": {},
   "source": [
    "## Preliminary Result\n",
    "\n",
    "### Control:\n",
    "**1. TfidfVectorizer(min_df=2, max_df=0.95)**\n",
    "\n",
    "**2. modelType=\"multinomial\"**\n",
    "\n",
    "###  Accuracy result captures:\n",
    "1. Control\n",
    "\n",
    "Train loss: 0.9160, accuracy: 78.52%; Testing loss: 1.2395, accuracy: 47.61%.\n",
    "\n",
    "2. min_df = 4\n",
    "\n",
    "Train loss: 0.9732, accuracy: 73.32; Testing loss: 1.2277, accuracy: 48.36%.\n",
    "\n",
    "3. min_df = 8\n",
    "\n",
    "Train loss: 1.0532, accuracy: 64.43%; Testing loss: 1.2383, accuracy: 47.36%.\n",
    "\n",
    "4. max_df = 0.75\n",
    "\n",
    "Train loss: 0.9160, accuracy: 78.52%; Testing loss: 1.2395, accuracy: 47.61%.\n",
    "\n",
    "5. max_df = 0.45\n",
    "\n",
    "Train loss: 0.9135, accuracy: 79.36%; Testing loss: 1.2402, accuracy: 47.61%.\n",
    "\n",
    "6. ngram_range=(1,2)\n",
    "\n",
    "Train loss: 0.8619, accuracy: 82.72%; Testing loss: 1.2345, accuracy: 48.61%.\n",
    "\n",
    "7. ngram_range=(1,3)\n",
    "\n",
    "Train loss: 0.8559, accuracy: 82.89%; Testing loss: 1.2367, accuracy: 47.10%.\n",
    "\n",
    "8. modelType=\"complement\"\n",
    "\n",
    "Train loss: 1.0443, accuracy: 89.60%; Testing loss: 1.2878, accuracy: 47.61%.\n",
    "\n",
    "###  Current best result:\n",
    "1. min_df = 4, max_df = 0.95, ngram_range=(1,2)\n",
    "\n",
    "Train loss: 0.9433, accuracy: 75.50%; Testing loss: 1.2211, accuracy: 50.63%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9dc9a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+--------------------+\n",
      "|      tf-idf vectors|label|prediction|         probability|\n",
      "+--------------------+-----+----------+--------------------+\n",
      "|[0.0,0.0,0.0,0.0,...|    0|       0.0|[0.50812020215687...|\n",
      "|[0.0,0.0,0.0,0.0,...|    0|       0.0|[0.30836712661241...|\n",
      "|[0.0,0.0,0.0,0.0,...|    0|       0.0|[0.48166239626815...|\n",
      "|[0.0,0.0,0.0,0.0,...|    0|       0.0|[0.37653776171522...|\n",
      "|[0.0,0.0,0.0,0.0,...|    0|       0.0|[0.37741271967632...|\n",
      "|[0.0,0.0,0.0,0.0,...|    0|       0.0|[0.31971914512349...|\n",
      "|[0.0,0.0,0.0,0.0,...|    0|       0.0|[0.42827943363745...|\n",
      "|[0.0,0.0,0.0,0.0,...|    0|       0.0|[0.39138085905693...|\n",
      "|[0.0,0.0,0.0,0.0,...|    0|       3.0|[0.25094732599449...|\n",
      "|[0.0,0.0,0.0,0.0,...|    0|       2.0|[0.29007920052226...|\n",
      "+--------------------+-----+----------+--------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# Show a few prediction results with probabilities\n",
    "test_predictions.select(col_name, \"label\", \"prediction\", \"probability\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52a58d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818f2cdd",
   "metadata": {},
   "source": [
    "## Appendix:\n",
    "### Methods to Optimize TF-IDF and Naive Bayes Models\n",
    "\n",
    "**TF-IDF Optimization:**\n",
    "- **Tune Parameters:**\n",
    "  - Adjust `max_features`, `min_df`, `max_df` in `TfidfVectorizer` to control vocabulary size and filter rare/common terms.\n",
    "  - Try different `ngram_range` values (e.g., `(1,2)` for unigrams and bigrams).\n",
    "  - Experiment with and without `stop_words='english'`.\n",
    "- **Text Preprocessing:**\n",
    "  - Normalize text (lowercase, remove punctuation, stemming/lemmatization).\n",
    "  - Remove or correct misspellings and special characters.\n",
    "\n",
    "**Naive Bayes Optimization:**\n",
    "- **Model Type:**\n",
    "  - Try different `modelType` options: `multinomial`, `bernoulli`, `complement`.\n",
    "- **Class Imbalance:**\n",
    "  - If classes are imbalanced, consider resampling or adjusting class weights.\n",
    "- **Feature Selection:**\n",
    "  - Remove low-importance features or use dimensionality reduction (e.g., PCA).\n",
    "\n",
    "**General Approaches:**\n",
    "- **Cross-Validation:**\n",
    "  - Use cross-validation to tune hyperparameters and avoid overfitting.\n",
    "- **Ensemble Methods:**\n",
    "  - Combine predictions from multiple models (e.g., voting, stacking).\n",
    "- **Error Analysis:**\n",
    "  - Analyze misclassified samples to improve preprocessing or feature engineering.\n",
    "\n",
    "**Example: Tuning TF-IDF and Naive Bayes**\n",
    "```python\n",
    "# Example: Try bigrams and different min_df\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df=4, max_df=0.75)\n",
    "train_tfidf_matrix = vectorizer.fit_transform(train_documents)\n",
    "test_tfidf_matrix = vectorizer.transform(test_documents)\n",
    "\n",
    "# Example: Try different Naive Bayes model types\n",
    "nb = NaiveBayes(featuresCol=col_name, labelCol=\"label\", modelType=\"multinomial\")\n",
    "model = nb.fit(train_spark_df)\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BigData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

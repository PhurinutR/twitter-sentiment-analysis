{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f98facd",
   "metadata": {},
   "source": [
    "# TF-IDF model for word-embedding (Requested)\n",
    "### Implementation: Scikit's TF-IDF\n",
    "Author: Marcus KWAN TH\n",
    "\n",
    "Last updated: 2025-11-17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f68641d",
   "metadata": {},
   "source": [
    "## 1. Prerequisite: Data extraction from `util`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e224587",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The purpose of this lines is to:\n",
    "1. Prevent PySpark from using a different Python interpreter\n",
    "2. Adding the root path to the sys context for the runtime to properly import util.preprocessing, otherwise, error will occur.\n",
    "\n",
    "Based on the testing with Jupyter Notebook.\n",
    "\"\"\"\n",
    "\n",
    "import sys, os\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# Add the root folder to sys.path before importing util package\n",
    "root_path = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if root_path not in sys.path:\n",
    "    sys.path.append(root_path)\n",
    "\n",
    "# Import all necessary library for word-vectorization\n",
    "from pyspark.sql import SparkSession\n",
    "from util.preprocessing import load_and_preprocess_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f9a12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/17 14:09:57 WARN Utils: Your hostname, Marcuss-MacBook-Air.local, resolves to a loopback address: 127.0.0.1; using 10.11.97.189 instead (on interface en0)\n",
      "25/11/17 14:09:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/17 14:09:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/17 14:09:57 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Session\n",
    "ss  = SparkSession.builder \\\n",
    "        .appName(\"Marcus TF-IDF Word Dimension Builder\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Add util.zip to PySpark context \n",
    "spark = ss.sparkContext.addPyFile(\"../util.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed068c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/17 14:09:58 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 596\n",
      "Number of testing samples: 397\n",
      "\n",
      "Training 1: wishin i could go to home depot to buy shit to build shit\n",
      "Testing 2: cold war black ops zombie be a damn hype!!!!!!\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess training and testing data from the util package\n",
    "train_df = load_and_preprocess_data('../Twitter_data/traindata7.csv')\n",
    "test_df = load_and_preprocess_data('../Twitter_data/testdata7.csv')\n",
    "\n",
    "# Convert Spark DataFrames to Pandas for TF-IDF processing\n",
    "train_pandas = train_df.toPandas()\n",
    "test_pandas = test_df.toPandas()\n",
    "\n",
    "# Extract documents and labels from training data\n",
    "train_documents = train_pandas.iloc[:, 0].astype(str).tolist()\n",
    "train_labels = train_pandas.iloc[:, 1].tolist()\n",
    "\n",
    "# Extract documents and labels from testing data\n",
    "test_documents = test_pandas.iloc[:, 0].astype(str).tolist()\n",
    "test_labels = test_pandas.iloc[:, 1].tolist()\n",
    "\n",
    "print(f\"Number of training samples: {len(train_documents)}\")\n",
    "print(f\"Number of testing samples: {len(test_documents)}\\n\")\n",
    "print(f\"Training 1: {train_documents[0]}\")\n",
    "print(f\"Testing 2: {test_documents[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b84f61a",
   "metadata": {},
   "source": [
    "## 2. TF-IDF Embedding Implementation\n",
    "### Vectorize the pre-processed data by words (vocab) into Pandas DF using TF-IDF.\n",
    "Libraries: Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed82fefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries for word-vectorization\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06710f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all documents for TF-IDF fitting\n",
    "all_documents = train_documents + test_documents\n",
    "\n",
    "# Fit TF-IDF on all documents and get the document-term matrix\n",
    "vectorizer = TfidfVectorizer()\n",
    "all_vocab_vectorizer = vectorizer.fit_transform(all_documents)\n",
    "all_vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Fit TF-IDF on train documents and get the document-term matrix\n",
    "vectorizer = TfidfVectorizer()\n",
    "train_vocab_vectorizer = vectorizer.fit_transform(train_documents)\n",
    "train_vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Fit TF-IDF on testing documents and get the document-term matrix\n",
    "vectorizer = TfidfVectorizer()\n",
    "test_vocab_vectorizer = vectorizer.fit_transform(test_documents)\n",
    "test_vocab = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54d5efc",
   "metadata": {},
   "source": [
    "## 3. Save the TD-IDF-vectorized words to dataframes.\n",
    "Libraries: Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b1c6667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries for saving data\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a644a799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All docs: Vocabulary size: 4046\n",
      "All docs: Matrix shape: (993, 4046)\n",
      "\n",
      "Train docs: Vocabulary size: 2969\n",
      "Train docs: Matrix shape: (596, 2969)\n",
      "\n",
      "Test docs: Vocabulary size: 2209\n",
      "Test docs: Matrix shape: (397, 2209)\n"
     ]
    }
   ],
   "source": [
    "# Check the shape to ensure all words are present\n",
    "print(f\"All docs: Vocabulary size: {len(all_vocab)}\")\n",
    "print(f\"All docs: Matrix shape: {all_vocab_vectorizer.shape}\\n\")\n",
    "print(f\"Train docs: Vocabulary size: {len(train_vocab)}\")\n",
    "print(f\"Train docs: Matrix shape: {train_vocab_vectorizer.shape}\\n\")\n",
    "print(f\"Test docs: Vocabulary size: {len(test_vocab)}\")\n",
    "print(f\"Test docs: Matrix shape: {test_vocab_vectorizer.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6cd7561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For all words: \n",
      "TF-IDF matrix: Columns are words, rows are dimensions\n",
      "\n",
      "      00  000  00303   01   06   07   08   09  0ezqmlg9ik  0wn3frahg  ...  \\\n",
      "0    0.0  0.0    0.0  0.0  0.0  0.0  0.0  0.0         0.0        0.0  ...   \n",
      "1    0.0  0.0    0.0  0.0  0.0  0.0  0.0  0.0         0.0        0.0  ...   \n",
      "2    0.0  0.0    0.0  0.0  0.0  0.0  0.0  0.0         0.0        0.0  ...   \n",
      "3    0.0  0.0    0.0  0.0  0.0  0.0  0.0  0.0         0.0        0.0  ...   \n",
      "4    0.0  0.0    0.0  0.0  0.0  0.0  0.0  0.0         0.0        0.0  ...   \n",
      "..   ...  ...    ...  ...  ...  ...  ...  ...         ...        ...  ...   \n",
      "988  0.0  0.0    0.0  0.0  0.0  0.0  0.0  0.0         0.0        0.0  ...   \n",
      "989  0.0  0.0    0.0  0.0  0.0  0.0  0.0  0.0         0.0        0.0  ...   \n",
      "990  0.0  0.0    0.0  0.0  0.0  0.0  0.0  0.0         0.0        0.0  ...   \n",
      "991  0.0  0.0    0.0  0.0  0.0  0.0  0.0  0.0         0.0        0.0  ...   \n",
      "992  0.0  0.0    0.0  0.0  0.0  0.0  0.0  0.0         0.0        0.0  ...   \n",
      "\n",
      "     zelda  zero  zion  zkvpbrx5ymk  zla  zoe  zombie  zone  zsmitty   ยบรฐ  \n",
      "0      0.0   0.0   0.0          0.0  0.0  0.0     0.0   0.0      0.0  0.0  \n",
      "1      0.0   0.0   0.0          0.0  0.0  0.0     0.0   0.0      0.0  0.0  \n",
      "2      0.0   0.0   0.0          0.0  0.0  0.0     0.0   0.0      0.0  0.0  \n",
      "3      0.0   0.0   0.0          0.0  0.0  0.0     0.0   0.0      0.0  0.0  \n",
      "4      0.0   0.0   0.0          0.0  0.0  0.0     0.0   0.0      0.0  0.0  \n",
      "..     ...   ...   ...          ...  ...  ...     ...   ...      ...  ...  \n",
      "988    0.0   0.0   0.0          0.0  0.0  0.0     0.0   0.0      0.0  0.0  \n",
      "989    0.0   0.0   0.0          0.0  0.0  0.0     0.0   0.0      0.0  0.0  \n",
      "990    0.0   0.0   0.0          0.0  0.0  0.0     0.0   0.0      0.0  0.0  \n",
      "991    0.0   0.0   0.0          0.0  0.0  0.0     0.0   0.0      0.0  0.0  \n",
      "992    0.0   0.0   0.0          0.0  0.0  0.0     0.0   0.0      0.0  0.0  \n",
      "\n",
      "[993 rows x 4046 columns]\n"
     ]
    }
   ],
   "source": [
    "# Convert the TF-IDF matrix to an array format and create a pandas DataFrame\n",
    "all_df_tfidf = pd.DataFrame(all_vocab_vectorizer.toarray(), columns=all_vocab)\n",
    "train_df_tfidf = pd.DataFrame(train_vocab_vectorizer.toarray(), columns=train_vocab)\n",
    "test_df_tfidf = pd.DataFrame(test_vocab_vectorizer.toarray(), columns=test_vocab)\n",
    "\n",
    "print(\"For all words: \\nTF-IDF matrix: Columns are words, rows are dimensions\\n\")\n",
    "print(all_df_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdef101",
   "metadata": {},
   "source": [
    "## 4. Export the TF-IDF vectors to .txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01fba53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 4046 word TF-IDF vectors to /Users/MarcussPC/Desktop/Temp/DSAI4205_Project/twitter-sentiment-analysis/model5/output/1_tfidf_word_vectors.txt\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "ALL Documents Word Vectors\n",
    "'''\n",
    "# Export each word and its TF-IDF vector (train documents) to a text file.\n",
    "output_path = os.path.join(root_path + \"/model5/output\", \"1_tfidf_word_vectors.txt\")\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "\n",
    "    for word in all_df_tfidf.columns:\n",
    "        vec = all_df_tfidf[word].to_list()\n",
    "        vec_str = \", \".join(f\"{v:.6f}\" for v in vec) # Format numbers with 6 dp; adjust if necessary.\n",
    "        f.write(f\"{word} [{vec_str}]\\n\")\n",
    "\n",
    "print(f\"Wrote {len(all_df_tfidf.columns)} word TF-IDF vectors to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10844c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 2969 word TF-IDF vectors to /Users/MarcussPC/Desktop/Temp/DSAI4205_Project/twitter-sentiment-analysis/model5/output/2_train_docs_tfidf_word_vectors.txt\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "TRAIN Documents Word Vectors\n",
    "'''\n",
    "# Export each word and its TF-IDF vector (train documents) to a text file.\n",
    "output_path = os.path.join(root_path + \"/model5/output\", \"2_train_docs_tfidf_word_vectors.txt\")\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "\n",
    "    for word in train_df_tfidf.columns:\n",
    "        vec = train_df_tfidf[word].to_list()\n",
    "        vec_str = \", \".join(f\"{v:.6f}\" for v in vec) # Format numbers with 6 dp; adjust if necessary.\n",
    "        f.write(f\"{word} [{vec_str}]\\n\")\n",
    "\n",
    "print(f\"Wrote {len(train_df_tfidf.columns)} word TF-IDF vectors to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5601d608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 2209 word TF-IDF vectors to /Users/MarcussPC/Desktop/Temp/DSAI4205_Project/twitter-sentiment-analysis/model5/output/3_test_docs_tfidf_word_vectors.txt\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "TEST Documents Word Vectors\n",
    "'''\n",
    "# Export each word and its TF-IDF vector (test documents) to a text file.\n",
    "output_path = os.path.join(root_path + \"/model5/output\", \"3_test_docs_tfidf_word_vectors.txt\")\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "\n",
    "    for word in test_df_tfidf.columns:\n",
    "        vec = test_df_tfidf[word].to_list()\n",
    "        vec_str = \", \".join(f\"{v:.6f}\" for v in vec) # Format numbers with 6 dp; adjust if necessary.\n",
    "        f.write(f\"{word} [{vec_str}]\\n\")\n",
    "\n",
    "print(f\"Wrote {len(test_df_tfidf.columns)} word TF-IDF vectors to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e617301",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BigData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "044a87be",
   "metadata": {},
   "source": [
    "# TF-IDF Embedding Implementation\n",
    "### Vectorize the pre-processed data into PySpark dataframe using TF-IDF.\n",
    "Libraries: Scikit-learn, PySpark\n",
    "\n",
    "Author: Marcus KWAN TH\n",
    "\n",
    "Last updated: 2025-11-14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f303eab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The use of this lines is to:\n",
    "1. Prevent PySpark from using a different Python interpreter\n",
    "2. Adding the root path to the sys context for the runtime to properly import util.preprocessing, otherwise, error will occur.\n",
    "based on the testing with Jupyter Notebook.\n",
    "\"\"\"\n",
    "\n",
    "import sys, os\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# Add the root folder to sys.path before importing util package\n",
    "root_path = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if root_path not in sys.path:\n",
    "    sys.path.append(root_path)\n",
    "\n",
    "from util.preprocessing import load_and_preprocess_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93fd1d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary library for TF-IDF embedding\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bed50ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/14 23:52:59 WARN Utils: Your hostname, Marcuss-MacBook-Air.local, resolves to a loopback address: 127.0.0.1; using 192.168.0.216 instead (on interface en0)\n",
      "25/11/14 23:52:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/14 23:52:59 WARN Utils: Your hostname, Marcuss-MacBook-Air.local, resolves to a loopback address: 127.0.0.1; using 192.168.0.216 instead (on interface en0)\n",
      "25/11/14 23:52:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/14 23:53:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/14 23:53:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Before running the following code, please ensure to zip the util folder using\n",
    "command: zip -r util.zip util\n",
    "on the root directory.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize Spark Session\n",
    "ss  = SparkSession.builder \\\n",
    "        .appName(\"Marcus TF-IDF\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Add util.zip to PySpark context \n",
    "spark = ss.sparkContext.addPyFile(\"../util.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07eac53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/14 23:53:01 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess training and testing data from the util package\n",
    "train_df = load_and_preprocess_data('../Twitter_data/traindata7.csv')\n",
    "test_df = load_and_preprocess_data('../Twitter_data/testdata7.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbe54d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Convert Spark DataFrames to Pandas for TF-IDF processing\n",
    "train_pandas = train_df.toPandas()\n",
    "test_pandas = test_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b19f3465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 596\n",
      "Testing samples: 397\n"
     ]
    }
   ],
   "source": [
    "# Extract documents and labels from training data\n",
    "train_documents = train_pandas.iloc[:, 0].astype(str).tolist()\n",
    "train_labels = train_pandas.iloc[:, 1].tolist()\n",
    "\n",
    "# Extract documents and labels from testing data\n",
    "test_documents = test_pandas.iloc[:, 0].astype(str).tolist()\n",
    "test_labels = test_pandas.iloc[:, 1].tolist()\n",
    "\n",
    "print(f\"Training samples: {len(train_documents)}\")\n",
    "print(f\"Testing samples: {len(test_documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2abcd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply IDF Vectorizer fit on training data\n",
    "vectorizer = TfidfVectorizer(max_features=1000, min_df=2, max_df=0.95)\n",
    "train_tfidf_matrix = vectorizer.fit_transform(train_documents)\n",
    "\n",
    "# Apply TF Vectorizer on testing data\n",
    "test_tfidf_matrix = vectorizer.transform(test_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89e54e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape for (1) Train: (596, 1000), (2) Test: (397, 1000)\n",
      "Vocabulary size: 1000\n"
     ]
    }
   ],
   "source": [
    "# Convert sparse (doc-term) matrices to dense arrays\n",
    "train_tfidf_dense = train_tfidf_matrix.toarray()\n",
    "test_tfidf_dense = test_tfidf_matrix.toarray()\n",
    "\n",
    "print(f\"TF-IDF matrix shape for (1) Train: {train_tfidf_dense.shape}, (2) Test: {test_tfidf_dense.shape}\")\n",
    "print(f\"Vocabulary size: {len(vectorizer.get_feature_names_out())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "211cbc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PySpark DataFrames for training and testing data for later stages\n",
    "train_spark_df = ss.createDataFrame(\n",
    "    [(Vectors.dense(vec), int(lbl)) for vec, lbl in zip(train_tfidf_dense, train_labels)],\n",
    "    [\"tf-idf\", \"label\"]\n",
    ")\n",
    "\n",
    "test_spark_df = ss.createDataFrame(\n",
    "    [(Vectors.dense(vec), int(lbl)) for vec, lbl in zip(test_tfidf_dense, test_labels)],\n",
    "    [\"tf-idf\", \"label\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9036821",
   "metadata": {},
   "source": [
    "# Naive Bayes Data Analytic Model Implementation\n",
    "### Sentiment classification using Naive Bayes model using PySpark.\n",
    "Libraries: Pyspark (classification and evaluator)\n",
    "\n",
    "Author: Marcus KWAN TH\n",
    "\n",
    "Last updated: 2025-11-14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19b2b605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for Naive Bayes classification\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dedde4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Train the Naive Bayes model on training data\n",
    "nb = NaiveBayes(featuresCol=\"tf-idf\", labelCol=\"label\", modelType=\"multinomial\")\n",
    "model = nb.fit(train_spark_df) # Currently no params set.\n",
    "\n",
    "# Perform prediction analysis on the fitted model\n",
    "predictions = model.transform(test_spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e1e63d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/14 23:53:07 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+--------------------+\n",
      "|              tf-idf|label|prediction|         probability|\n",
      "+--------------------+-----+----------+--------------------+\n",
      "|[0.0,0.0,0.0,0.0,...|    0|       0.0|[0.47656605906150...|\n",
      "|[0.0,0.0,0.0,0.0,...|    0|       1.0|[0.30388306807292...|\n",
      "|[0.0,0.0,0.0,0.0,...|    0|       1.0|[0.37299478086017...|\n",
      "|[0.0,0.0,0.0,0.0,...|    0|       0.0|[0.40872759652428...|\n",
      "|[0.0,0.0,0.0,0.0,...|    0|       0.0|[0.37958369959263...|\n",
      "+--------------------+-----+----------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Show a few prediction results with probabilities (5)\n",
    "predictions.select(\"tf-idf\", \"label\", \"prediction\", \"probability\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7903a5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.4761\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the accuracy using PySpark (for now)\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "accuracy = accuracy_evaluator.evaluate(predictions)\n",
    "print(f\"Test set accuracy = {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BigData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

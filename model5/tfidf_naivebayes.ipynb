{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65121d22",
   "metadata": {},
   "source": [
    "# TFIDF-Naive Bayes Model for Twitter Sentiment Analysis\n",
    "Author: Marcus KWAN TH\n",
    "\n",
    "Last updated: 2025-11-15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044a87be",
   "metadata": {},
   "source": [
    "## 1. TF-IDF Embedding Implementation\n",
    "### Vectorize the pre-processed data into PySpark dataframe using TF-IDF.\n",
    "Libraries: Scikit-learn, PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f303eab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The use of this lines is to:\n",
    "1. Prevent PySpark from using a different Python interpreter\n",
    "2. Adding the root path to the sys context for the runtime to properly import util.preprocessing, otherwise, error will occur.\n",
    "based on the testing with Jupyter Notebook.\n",
    "\"\"\"\n",
    "\n",
    "import sys, os\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# Add the root folder to sys.path before importing util package\n",
    "root_path = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if root_path not in sys.path:\n",
    "    sys.path.append(root_path)\n",
    "\n",
    "from util.preprocessing import load_and_preprocess_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93fd1d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary library for TF-IDF embedding\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bed50ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/15 18:04:03 WARN Utils: Your hostname, Marcuss-MacBook-Air.local, resolves to a loopback address: 127.0.0.1; using 192.168.0.216 instead (on interface en0)\n",
      "25/11/15 18:04:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/15 18:04:03 WARN Utils: Your hostname, Marcuss-MacBook-Air.local, resolves to a loopback address: 127.0.0.1; using 192.168.0.216 instead (on interface en0)\n",
      "25/11/15 18:04:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/15 18:04:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/15 18:04:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Before running the following code, please ensure to zip the util folder using\n",
    "command: zip -r util.zip util\n",
    "on the root directory.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize Spark Session\n",
    "ss  = SparkSession.builder \\\n",
    "        .appName(\"Marcus TF-IDF Naive-Bayes Model\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Add util.zip to PySpark context \n",
    "spark = ss.sparkContext.addPyFile(\"../util.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c69066cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables:\n",
    "col_name = \"tf-idf vectors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07eac53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/15 18:04:05 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess training and testing data from the util package\n",
    "train_df = load_and_preprocess_data('../Twitter_data/traindata7.csv')\n",
    "test_df = load_and_preprocess_data('../Twitter_data/testdata7.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbe54d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Convert Spark DataFrames to Pandas for TF-IDF processing\n",
    "train_pandas = train_df.toPandas()\n",
    "test_pandas = test_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b19f3465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 596\n",
      "Number of testing samples: 397\n",
      "\n",
      "Training 1: wishin i could go to home depot to buy shit to build shit\n",
      "Testing 2: cold war black ops zombie be a damn hype!!!!!!\n"
     ]
    }
   ],
   "source": [
    "# Extract documents and labels from training data\n",
    "train_documents = train_pandas.iloc[:, 0].astype(str).tolist()\n",
    "train_labels = train_pandas.iloc[:, 1].tolist()\n",
    "\n",
    "# Extract documents and labels from testing data\n",
    "test_documents = test_pandas.iloc[:, 0].astype(str).tolist()\n",
    "test_labels = test_pandas.iloc[:, 1].tolist()\n",
    "\n",
    "print(f\"Number of training samples: {len(train_documents)}\")\n",
    "print(f\"Number of testing samples: {len(test_documents)}\\n\")\n",
    "\n",
    "print(f\"Training 1: {train_documents[0]}\")\n",
    "print(f\"Testing 2: {test_documents[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2abcd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply IDF Vectorizer fit on training data\n",
    "vectorizer = TfidfVectorizer(min_df=4, max_df=0.95 ,ngram_range=(1,2))\n",
    "train_tfidf_matrix = vectorizer.fit_transform(train_documents)\n",
    "\n",
    "# Apply TF Vectorizer on testing data\n",
    "test_tfidf_matrix = vectorizer.transform(test_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89e54e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape for (1) Train: (596, 673), (2) Test: (397, 673)\n",
      "Vocabulary size: 673\n"
     ]
    }
   ],
   "source": [
    "# Convert sparse (doc-term) matrices to dense arrays\n",
    "train_tfidf_dense = train_tfidf_matrix.toarray()\n",
    "test_tfidf_dense = test_tfidf_matrix.toarray()\n",
    "\n",
    "print(f\"TF-IDF matrix shape for (1) Train: {train_tfidf_dense.shape}, (2) Test: {test_tfidf_dense.shape}\")\n",
    "print(f\"Vocabulary size: {len(vectorizer.get_feature_names_out())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "211cbc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PySpark DataFrames for training and testing data for later stages\n",
    "train_spark_df = ss.createDataFrame(\n",
    "    [(Vectors.dense(vec), int(lbl)) for vec, lbl in zip(train_tfidf_dense, train_labels)],\n",
    "    [col_name, \"label\"]\n",
    ")\n",
    "\n",
    "test_spark_df = ss.createDataFrame(\n",
    "    [(Vectors.dense(vec), int(lbl)) for vec, lbl in zip(test_tfidf_dense, test_labels)],\n",
    "    [col_name, \"label\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9036821",
   "metadata": {},
   "source": [
    "## 2. Naive Bayes Data Analytic Model Implementation\n",
    "### Sentiment classification using Naive Bayes model using PySpark.\n",
    "Libraries: Pyspark (Naive Bayes classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19b2b605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for Naive Bayes classification\n",
    "from pyspark.ml.classification import NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6dedde4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Train the Naive Bayes model on training data\n",
    "nb = NaiveBayes(featuresCol=col_name, labelCol=\"label\", modelType=\"multinomial\")\n",
    "model = nb.fit(train_spark_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244607a0",
   "metadata": {},
   "source": [
    "## 3. Simple Evaluation\n",
    "### Examine the performance of the TFIDF-NaivaBayes combination with training and testing loss with accuracy.\n",
    "Libraries: Pyspark (UDF and evaluation), Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1f97b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for evaluation\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "878d4e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract probability of the true class\n",
    "def get_prob(probability, label):\n",
    "    return float(probability[int(label)])\n",
    "get_prob_udf = udf(get_prob, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "091357ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/15 18:04:11 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Loss Evaluation\n",
    "\"\"\"\n",
    "# Set selectExpr statement\n",
    "statement = \"mean(log(true_prob)) as log_loss\"\n",
    "\n",
    "# Training loss\n",
    "train_predictions = model.transform(train_spark_df)\n",
    "train_predictions = train_predictions.withColumn(\"true_prob\", get_prob_udf(col(\"probability\"), col(\"label\")))\n",
    "train_loss = -train_predictions.selectExpr(statement).collect()[0][\"log_loss\"]\n",
    "\n",
    "# Testing loss\n",
    "test_predictions = model.transform(test_spark_df)\n",
    "test_predictions = test_predictions.withColumn(\"true_prob\", get_prob_udf(col(\"probability\"), col(\"label\")))\n",
    "test_loss = -test_predictions.selectExpr(statement).collect()[0][\"log_loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7903a5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.9433\n",
      "Testing Loss: 1.2211\n",
      "\n",
      "Train set accuracy = 0.7550\n",
      "Test set accuracy = 0.5063\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Accuracy Evaluation\n",
    "\"\"\"\n",
    "# Evaluate the accuracy using PySpark\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "train_accuracy = accuracy_evaluator.evaluate(train_predictions)\n",
    "test_accuracy = accuracy_evaluator.evaluate(test_predictions)\n",
    "\n",
    "# Result\n",
    "print(f\"Training Loss: {train_loss:.4f}\")\n",
    "print(f\"Testing Loss: {test_loss:.4f}\\n\")\n",
    "print(f\"Train set accuracy = {train_accuracy:.4f}\")\n",
    "print(f\"Test set accuracy = {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c424d36",
   "metadata": {},
   "source": [
    "## 4. Preliminary Result\n",
    "\n",
    "### Control:\n",
    "**1. TfidfVectorizer(min_df=2, max_df=0.95)**\n",
    "\n",
    "**2. modelType=\"multinomial\"**\n",
    "\n",
    "###  Accuracy result captures:\n",
    "1. Base\n",
    "\n",
    "Train loss: 0.9160, accuracy: 78.52%; Testing loss: 1.2395, accuracy: 47.61%.\n",
    "\n",
    "2. min_df = 4\n",
    "\n",
    "Train loss: 0.9732, accuracy: 73.32; Testing loss: 1.2277, accuracy: 48.36%.\n",
    "\n",
    "3. max_df = 0.75\n",
    "\n",
    "Train loss: 0.9160, accuracy: 78.52%; Testing loss: 1.2395, accuracy: 47.61%.\n",
    "\n",
    "4. ngram_range=(1,2)\n",
    "\n",
    "Train loss: 0.8619, accuracy: 82.72; Testing loss: 1.2345, accuracy: 48.61%.\n",
    "\n",
    "5. modelType=\"complement\"\n",
    "\n",
    "Train loss: 1.0443, accuracy: 89.60%; Testing loss: 1.2878, accuracy: 47.61%.\n",
    "\n",
    "###  Current best result:\n",
    "1. min_df = 4, ngram_range=(1,2)\n",
    "\n",
    "Train loss: 0.9433, accuracy: 75.50%; Testing loss: 1.2211, accuracy: 50.63%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9dc9a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+--------------------+\n",
      "|      tf-idf vectors|label|prediction|         probability|\n",
      "+--------------------+-----+----------+--------------------+\n",
      "|[0.0,0.0,0.0,0.0,...|    0|       0.0|[0.50812020215687...|\n",
      "|[0.0,0.0,0.0,0.0,...|    0|       0.0|[0.30836712661241...|\n",
      "|[0.0,0.0,0.0,0.0,...|    0|       0.0|[0.48166239626815...|\n",
      "|[0.0,0.0,0.0,0.0,...|    0|       0.0|[0.37653776171522...|\n",
      "|[0.0,0.0,0.0,0.0,...|    0|       0.0|[0.37741271967632...|\n",
      "|[0.0,0.0,0.0,0.0,...|    0|       0.0|[0.31971914512349...|\n",
      "|[0.0,0.0,0.0,0.0,...|    0|       0.0|[0.42827943363745...|\n",
      "|[0.0,0.0,0.0,0.0,...|    0|       0.0|[0.39138085905693...|\n",
      "|[0.0,0.0,0.0,0.0,...|    0|       3.0|[0.25094732599449...|\n",
      "|[0.0,0.0,0.0,0.0,...|    0|       2.0|[0.29007920052226...|\n",
      "+--------------------+-----+----------+--------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# Show a few prediction results with probabilities\n",
    "test_predictions.select(col_name, \"label\", \"prediction\", \"probability\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818f2cdd",
   "metadata": {},
   "source": [
    "## Appendix:\n",
    "### Methods to Optimize TF-IDF and Naive Bayes Models\n",
    "\n",
    "**TF-IDF Optimization:**\n",
    "- **Tune Parameters:**\n",
    "  - Adjust `max_features`, `min_df`, `max_df` in `TfidfVectorizer` to control vocabulary size and filter rare/common terms.\n",
    "  - Try different `ngram_range` values (e.g., `(1,2)` for unigrams and bigrams).\n",
    "  - Experiment with and without `stop_words='english'`.\n",
    "- **Text Preprocessing:**\n",
    "  - Normalize text (lowercase, remove punctuation, stemming/lemmatization).\n",
    "  - Remove or correct misspellings and special characters.\n",
    "\n",
    "**Naive Bayes Optimization:**\n",
    "- **Model Type:**\n",
    "  - Try different `modelType` options: `multinomial`, `bernoulli`, `complement`.\n",
    "- **Class Imbalance:**\n",
    "  - If classes are imbalanced, consider resampling or adjusting class weights.\n",
    "- **Feature Selection:**\n",
    "  - Remove low-importance features or use dimensionality reduction (e.g., PCA).\n",
    "\n",
    "**General Approaches:**\n",
    "- **Cross-Validation:**\n",
    "  - Use cross-validation to tune hyperparameters and avoid overfitting.\n",
    "- **Ensemble Methods:**\n",
    "  - Combine predictions from multiple models (e.g., voting, stacking).\n",
    "- **Error Analysis:**\n",
    "  - Analyze misclassified samples to improve preprocessing or feature engineering.\n",
    "\n",
    "**Example: Tuning TF-IDF and Naive Bayes**\n",
    "```python\n",
    "# Example: Try bigrams and different min_df\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df=4, max_df=0.75)\n",
    "train_tfidf_matrix = vectorizer.fit_transform(train_documents)\n",
    "test_tfidf_matrix = vectorizer.transform(test_documents)\n",
    "\n",
    "# Example: Try different Naive Bayes model types\n",
    "nb = NaiveBayes(featuresCol=col_name, labelCol=\"label\", modelType=\"multinomial\")\n",
    "model = nb.fit(train_spark_df)\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BigData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

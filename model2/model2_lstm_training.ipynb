{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99854bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fe34728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "TEXT = data.Field(tokenize = 'spacy',\n",
    "                  tokenizer_language = 'en_core_web_trf', # en_core_web_md , en_core_web_lg\n",
    "                  include_lengths = True)\n",
    "LABEL = data.LabelField(dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4f61c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Use torchtext.legacy.data if it exists, otherwise torchtext.data\n",
    "# ---------------------------------------------------------------\n",
    "try:                     # torchtext >= 0.9\n",
    "    from torchtext.legacy import data \n",
    "except ImportError:      # torchtext <= 0.8 (your 0.5.0 case)\n",
    "    from torchtext import data\n",
    "\n",
    "class FolderDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    root/\n",
    "      ├── train/\n",
    "      │     ├── classA/\n",
    "      │     │     ├── xxx.txt\n",
    "      │     │     └── yyy.txt\n",
    "      │     └── classB/\n",
    "      └── test/\n",
    "            └── ...\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def sort_key(ex):            # lets BucketIterator bucket by length\n",
    "        return len(ex.text)\n",
    "\n",
    "    def __init__(self, root, text_field, label_field,\n",
    "                 encoding=\"utf-8\", **kwargs):\n",
    "        fields   = [(\"text\", text_field), (\"label\", label_field)]\n",
    "        examples = []\n",
    "\n",
    "        root = pathlib.Path(root)\n",
    "        for class_dir in sorted(p for p in root.iterdir() if p.is_dir()):\n",
    "            label = class_dir.name                      # folder name → label\n",
    "            for fp in class_dir.glob(\"*.txt\"):\n",
    "                txt = fp.read_text(encoding=encoding)\n",
    "                examples.append(data.Example.fromlist([txt, label], fields))\n",
    "\n",
    "        super(FolderDataset, self).__init__(examples, fields, **kwargs)\n",
    "\n",
    "    # ------------ helper that mimics datasets.IMDB.splits ------------\n",
    "    @classmethod\n",
    "    def splits(cls, text_field, label_field,\n",
    "               path, train=\"train\", test=\"test\", **kwargs):\n",
    "        train_ds = cls(os.path.join(path, train),\n",
    "                       text_field, label_field, **kwargs)\n",
    "        test_ds  = cls(os.path.join(path, test),\n",
    "                       text_field, label_field, **kwargs)\n",
    "        return train_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa3e572d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train examples : 58907\n",
      "test  examples : 397\n"
     ]
    }
   ],
   "source": [
    "from torchtext import data            # 0.5 import\n",
    "\n",
    "# 2) Datasets ----------------------------------------------------------\n",
    "train_data, test_data = FolderDataset.splits(\n",
    "        TEXT, LABEL,\n",
    "        path=\"data\"          # <- folder that contains /train and /test\n",
    ")\n",
    "\n",
    "print(\"train examples :\", len(train_data))\n",
    "print(\"test  examples :\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ae65acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 250_000\n",
    "TEXT.build_vocab(train_data,\n",
    "                 max_size = MAX_VOCAB_SIZE,\n",
    "                 vectors = \"glove.840B.300d\",       #\"glove.6B.100d\",glove.840B.300d  glove.42B.300d  \"glove.twitter.27B.200d\"     # Do not use complicated word-embedding to reduce download time\n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "LABEL.build_vocab(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cc97838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 55874), ('the', 35601), ('be', 32888), ('i', 29555), (',', 25508), ('!', 23605), ('to', 22903), ('a', 21937), ('and', 21301), ('of', 15566), ('it', 15334), ('for', 12538), ('/', 12502), ('in', 12349), ('this', 11820), ('have', 10724), ('you', 10471), ('?', 10175), ('...', 9835), ('on', 9679)]\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.freqs.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58a0a0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1302628\n"
     ]
    }
   ],
   "source": [
    "total_tokens = sum(TEXT.vocab.freqs.values())\n",
    "print(total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ffbba6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, test_data),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_within_batch = True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a0c5490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model | layers=2, bidirectional=True, emb_dim=4500, dropout=0.7\n",
      "Epoch 01 | Train: loss=1.197 acc=48.05% | Test: loss=0.958 acc=61.50% | time: 0m 25s | best-val-loss: 0.958 (epoch 1)| best-val-acc: 61.50% (epoch 1)\n",
      "Epoch 02 | Train: loss=1.055 acc=56.48% | Test: loss=0.804 acc=71.72% | time: 0m 24s | best-val-loss: 0.804 (epoch 2)| best-val-acc: 71.72% (epoch 2)\n",
      "Epoch 03 | Train: loss=0.982 acc=59.93% | Test: loss=0.748 acc=69.76% | time: 0m 24s | best-val-loss: 0.748 (epoch 3)| best-val-acc: 71.72% (epoch 2)\n",
      "Epoch 04 | Train: loss=0.924 acc=63.01% | Test: loss=0.690 acc=75.31% | time: 0m 24s | best-val-loss: 0.690 (epoch 4)| best-val-acc: 75.31% (epoch 4)\n",
      "Epoch 05 | Train: loss=0.875 acc=65.23% | Test: loss=0.659 acc=76.00% | time: 0m 24s | best-val-loss: 0.659 (epoch 5)| best-val-acc: 76.00% (epoch 5)\n",
      "Epoch 06 | Train: loss=0.836 acc=67.00% | Test: loss=0.608 acc=76.01% | time: 0m 24s | best-val-loss: 0.608 (epoch 6)| best-val-acc: 76.01% (epoch 6)\n",
      "Epoch 07 | Train: loss=0.798 acc=68.81% | Test: loss=0.535 acc=81.11% | time: 0m 24s | best-val-loss: 0.535 (epoch 7)| best-val-acc: 81.11% (epoch 7)\n",
      "Epoch 08 | Train: loss=0.766 acc=70.28% | Test: loss=0.476 acc=80.67% | time: 0m 24s | best-val-loss: 0.476 (epoch 8)| best-val-acc: 81.11% (epoch 7)\n",
      "Epoch 09 | Train: loss=0.733 acc=71.71% | Test: loss=0.461 acc=85.32% | time: 0m 24s | best-val-loss: 0.461 (epoch 9)| best-val-acc: 85.32% (epoch 9)\n",
      "Epoch 10 | Train: loss=0.709 acc=72.74% | Test: loss=0.474 acc=80.89% | time: 0m 24s | best-val-loss: 0.461 (epoch 9)| best-val-acc: 85.32% (epoch 9)\n",
      "Epoch 11 | Train: loss=0.680 acc=74.07% | Test: loss=0.451 acc=85.11% | time: 0m 24s | best-val-loss: 0.451 (epoch 11)| best-val-acc: 85.32% (epoch 9)\n",
      "Epoch 12 | Train: loss=0.663 acc=74.90% | Test: loss=0.474 acc=84.24% | time: 0m 24s | best-val-loss: 0.451 (epoch 11)| best-val-acc: 85.32% (epoch 9)\n",
      "Epoch 13 | Train: loss=0.636 acc=76.02% | Test: loss=0.384 acc=86.90% | time: 0m 24s | best-val-loss: 0.384 (epoch 13)| best-val-acc: 86.90% (epoch 13)\n",
      "Epoch 14 | Train: loss=0.619 acc=76.53% | Test: loss=0.354 acc=86.68% | time: 0m 23s | best-val-loss: 0.354 (epoch 14)| best-val-acc: 86.90% (epoch 13)\n",
      "Epoch 15 | Train: loss=0.600 acc=77.37% | Test: loss=0.396 acc=86.02% | time: 0m 20s | best-val-loss: 0.354 (epoch 14)| best-val-acc: 86.90% (epoch 13)\n",
      "Epoch 16 | Train: loss=0.584 acc=78.06% | Test: loss=0.411 acc=86.92% | time: 0m 24s | best-val-loss: 0.354 (epoch 14)| best-val-acc: 86.92% (epoch 16)\n",
      "Epoch 17 | Train: loss=0.567 acc=78.75% | Test: loss=0.405 acc=87.57% | time: 0m 24s | best-val-loss: 0.354 (epoch 14)| best-val-acc: 87.57% (epoch 17)\n",
      "Epoch 18 | Train: loss=0.555 acc=79.35% | Test: loss=0.320 acc=88.24% | time: 0m 24s | best-val-loss: 0.320 (epoch 18)| best-val-acc: 88.24% (epoch 18)\n",
      "Epoch 19 | Train: loss=0.539 acc=80.01% | Test: loss=0.355 acc=88.89% | time: 0m 24s | best-val-loss: 0.320 (epoch 18)| best-val-acc: 88.89% (epoch 19)\n",
      "Epoch 20 | Train: loss=0.532 acc=80.09% | Test: loss=0.316 acc=90.23% | time: 0m 24s | best-val-loss: 0.316 (epoch 20)| best-val-acc: 90.23% (epoch 20)\n",
      "Epoch 21 | Train: loss=0.513 acc=80.84% | Test: loss=0.373 acc=86.26% | time: 0m 24s | best-val-loss: 0.316 (epoch 20)| best-val-acc: 90.23% (epoch 20)\n",
      "Epoch 22 | Train: loss=0.504 acc=81.16% | Test: loss=0.306 acc=89.80% | time: 0m 24s | best-val-loss: 0.306 (epoch 22)| best-val-acc: 90.23% (epoch 20)\n",
      "Epoch 23 | Train: loss=0.498 acc=81.38% | Test: loss=0.251 acc=90.92% | time: 0m 24s | best-val-loss: 0.251 (epoch 23)| best-val-acc: 90.92% (epoch 23)\n",
      "Epoch 24 | Train: loss=0.482 acc=81.96% | Test: loss=0.269 acc=92.89% | time: 0m 24s | best-val-loss: 0.251 (epoch 23)| best-val-acc: 92.89% (epoch 24)\n",
      "Epoch 25 | Train: loss=0.475 acc=82.55% | Test: loss=0.295 acc=90.49% | time: 0m 24s | best-val-loss: 0.251 (epoch 23)| best-val-acc: 92.89% (epoch 24)\n",
      "Epoch 26 | Train: loss=0.465 acc=82.83% | Test: loss=0.273 acc=92.69% | time: 0m 24s | best-val-loss: 0.251 (epoch 23)| best-val-acc: 92.89% (epoch 24)\n",
      "Epoch 27 | Train: loss=0.462 acc=82.98% | Test: loss=0.299 acc=91.14% | time: 0m 24s | best-val-loss: 0.251 (epoch 23)| best-val-acc: 92.89% (epoch 24)\n",
      "Epoch 28 | Train: loss=0.450 acc=83.35% | Test: loss=0.309 acc=89.15% | time: 0m 24s | best-val-loss: 0.251 (epoch 23)| best-val-acc: 92.89% (epoch 24)\n",
      "Epoch 29 | Train: loss=0.439 acc=83.71% | Test: loss=0.421 acc=87.81% | time: 0m 24s | best-val-loss: 0.251 (epoch 23)| best-val-acc: 92.89% (epoch 24)\n",
      "Epoch 30 | Train: loss=0.437 acc=83.94% | Test: loss=0.328 acc=90.25% | time: 0m 23s | best-val-loss: 0.251 (epoch 23)| best-val-acc: 92.89% (epoch 24)\n",
      "Epoch 31 | Train: loss=0.431 acc=83.97% | Test: loss=0.300 acc=89.37% | time: 0m 23s | best-val-loss: 0.251 (epoch 23)| best-val-acc: 92.89% (epoch 24)\n",
      "Epoch 32 | Train: loss=0.425 acc=84.33% | Test: loss=0.306 acc=90.02% | time: 0m 23s | best-val-loss: 0.251 (epoch 23)| best-val-acc: 92.89% (epoch 24)\n",
      "Epoch 33 | Train: loss=0.413 acc=84.68% | Test: loss=0.231 acc=92.24% | time: 0m 23s | best-val-loss: 0.231 (epoch 33)| best-val-acc: 92.89% (epoch 24)\n",
      "Epoch 34 | Train: loss=0.408 acc=84.97% | Test: loss=0.213 acc=92.02% | time: 0m 23s | best-val-loss: 0.213 (epoch 34)| best-val-acc: 92.89% (epoch 24)\n",
      "Epoch 35 | Train: loss=0.401 acc=85.27% | Test: loss=0.269 acc=89.37% | time: 0m 23s | best-val-loss: 0.213 (epoch 34)| best-val-acc: 92.89% (epoch 24)\n",
      "Epoch 36 | Train: loss=0.399 acc=85.26% | Test: loss=0.268 acc=92.24% | time: 0m 24s | best-val-loss: 0.213 (epoch 34)| best-val-acc: 92.89% (epoch 24)\n",
      "Epoch 37 | Train: loss=0.396 acc=85.60% | Test: loss=0.190 acc=92.02% | time: 0m 24s | best-val-loss: 0.190 (epoch 37)| best-val-acc: 92.89% (epoch 24)\n",
      "Epoch 38 | Train: loss=0.388 acc=85.77% | Test: loss=0.186 acc=92.69% | time: 0m 24s | best-val-loss: 0.186 (epoch 38)| best-val-acc: 92.89% (epoch 24)\n",
      "Epoch 39 | Train: loss=0.384 acc=85.91% | Test: loss=0.220 acc=92.69% | time: 0m 23s | best-val-loss: 0.186 (epoch 38)| best-val-acc: 92.89% (epoch 24)\n",
      "Epoch 40 | Train: loss=0.375 acc=86.25% | Test: loss=0.215 acc=92.91% | time: 0m 24s | best-val-loss: 0.186 (epoch 38)| best-val-acc: 92.91% (epoch 40)\n",
      "Epoch 41 | Train: loss=0.375 acc=86.30% | Test: loss=0.231 acc=93.36% | time: 0m 24s | best-val-loss: 0.186 (epoch 38)| best-val-acc: 93.36% (epoch 41)\n",
      "Epoch 42 | Train: loss=0.369 acc=86.46% | Test: loss=0.273 acc=92.69% | time: 0m 24s | best-val-loss: 0.186 (epoch 38)| best-val-acc: 93.36% (epoch 41)\n",
      "Epoch 43 | Train: loss=0.362 acc=86.74% | Test: loss=0.185 acc=95.12% | time: 0m 24s | best-val-loss: 0.185 (epoch 43)| best-val-acc: 95.12% (epoch 43)\n",
      "Epoch 44 | Train: loss=0.362 acc=86.63% | Test: loss=0.234 acc=93.58% | time: 0m 23s | best-val-loss: 0.185 (epoch 43)| best-val-acc: 95.12% (epoch 43)\n",
      "Epoch 45 | Train: loss=0.354 acc=86.91% | Test: loss=0.207 acc=93.80% | time: 0m 23s | best-val-loss: 0.185 (epoch 43)| best-val-acc: 95.12% (epoch 43)\n",
      "Epoch 46 | Train: loss=0.355 acc=86.89% | Test: loss=0.234 acc=94.02% | time: 0m 24s | best-val-loss: 0.185 (epoch 43)| best-val-acc: 95.12% (epoch 43)\n",
      "Epoch 47 | Train: loss=0.352 acc=87.07% | Test: loss=0.253 acc=92.93% | time: 0m 24s | best-val-loss: 0.185 (epoch 43)| best-val-acc: 95.12% (epoch 43)\n",
      "Epoch 48 | Train: loss=0.347 acc=87.23% | Test: loss=0.231 acc=93.58% | time: 0m 24s | best-val-loss: 0.185 (epoch 43)| best-val-acc: 95.12% (epoch 43)\n",
      "Epoch 49 | Train: loss=0.344 acc=87.45% | Test: loss=0.206 acc=94.02% | time: 0m 24s | best-val-loss: 0.185 (epoch 43)| best-val-acc: 95.12% (epoch 43)\n",
      "Epoch 50 | Train: loss=0.345 acc=87.25% | Test: loss=0.145 acc=95.35% | time: 0m 24s | best-val-loss: 0.145 (epoch 50)| best-val-acc: 95.35% (epoch 50)\n",
      "Epoch 51 | Train: loss=0.334 acc=87.65% | Test: loss=0.252 acc=93.58% | time: 0m 23s | best-val-loss: 0.145 (epoch 50)| best-val-acc: 95.35% (epoch 50)\n",
      "Epoch 52 | Train: loss=0.331 acc=87.86% | Test: loss=0.222 acc=94.25% | time: 0m 24s | best-val-loss: 0.145 (epoch 50)| best-val-acc: 95.35% (epoch 50)\n",
      "Epoch 53 | Train: loss=0.330 acc=87.88% | Test: loss=0.152 acc=94.92% | time: 0m 24s | best-val-loss: 0.145 (epoch 50)| best-val-acc: 95.35% (epoch 50)\n",
      "Epoch 54 | Train: loss=0.325 acc=88.05% | Test: loss=0.214 acc=94.69% | time: 0m 23s | best-val-loss: 0.145 (epoch 50)| best-val-acc: 95.35% (epoch 50)\n",
      "Epoch 55 | Train: loss=0.326 acc=87.94% | Test: loss=0.195 acc=94.47% | time: 0m 24s | best-val-loss: 0.145 (epoch 50)| best-val-acc: 95.35% (epoch 50)\n",
      "Epoch 56 | Train: loss=0.322 acc=88.11% | Test: loss=0.268 acc=94.25% | time: 0m 23s | best-val-loss: 0.145 (epoch 50)| best-val-acc: 95.35% (epoch 50)\n",
      "Epoch 57 | Train: loss=0.319 acc=88.36% | Test: loss=0.165 acc=94.69% | time: 0m 24s | best-val-loss: 0.145 (epoch 50)| best-val-acc: 95.35% (epoch 50)\n",
      "Epoch 58 | Train: loss=0.315 acc=88.31% | Test: loss=0.137 acc=94.92% | time: 0m 23s | best-val-loss: 0.137 (epoch 58)| best-val-acc: 95.35% (epoch 50)\n",
      "Epoch 59 | Train: loss=0.316 acc=88.37% | Test: loss=0.162 acc=96.02% | time: 0m 24s | best-val-loss: 0.137 (epoch 58)| best-val-acc: 96.02% (epoch 59)\n",
      "Epoch 60 | Train: loss=0.314 acc=88.53% | Test: loss=0.235 acc=94.25% | time: 0m 24s | best-val-loss: 0.137 (epoch 58)| best-val-acc: 96.02% (epoch 59)\n",
      "Epoch 61 | Train: loss=0.308 acc=88.64% | Test: loss=0.255 acc=93.80% | time: 0m 24s | best-val-loss: 0.137 (epoch 58)| best-val-acc: 96.02% (epoch 59)\n",
      "Epoch 62 | Train: loss=0.304 acc=88.88% | Test: loss=0.287 acc=94.25% | time: 0m 24s | best-val-loss: 0.137 (epoch 58)| best-val-acc: 96.02% (epoch 59)\n",
      "Epoch 63 | Train: loss=0.302 acc=88.92% | Test: loss=0.162 acc=96.02% | time: 0m 24s | best-val-loss: 0.137 (epoch 58)| best-val-acc: 96.02% (epoch 59)\n",
      "Epoch 64 | Train: loss=0.301 acc=88.89% | Test: loss=0.214 acc=95.12% | time: 0m 24s | best-val-loss: 0.137 (epoch 58)| best-val-acc: 96.02% (epoch 59)\n",
      "Epoch 65 | Train: loss=0.300 acc=88.94% | Test: loss=0.209 acc=94.69% | time: 0m 24s | best-val-loss: 0.137 (epoch 58)| best-val-acc: 96.02% (epoch 59)\n",
      "Epoch 66 | Train: loss=0.297 acc=88.94% | Test: loss=0.178 acc=94.02% | time: 0m 24s | best-val-loss: 0.137 (epoch 58)| best-val-acc: 96.02% (epoch 59)\n",
      "Epoch 67 | Train: loss=0.295 acc=89.14% | Test: loss=0.216 acc=95.35% | time: 0m 24s | best-val-loss: 0.137 (epoch 58)| best-val-acc: 96.02% (epoch 59)\n",
      "Epoch 68 | Train: loss=0.295 acc=89.06% | Test: loss=0.168 acc=95.35% | time: 0m 24s | best-val-loss: 0.137 (epoch 58)| best-val-acc: 96.02% (epoch 59)\n",
      "Epoch 69 | Train: loss=0.290 acc=89.46% | Test: loss=0.149 acc=95.57% | time: 0m 24s | best-val-loss: 0.137 (epoch 58)| best-val-acc: 96.02% (epoch 59)\n",
      "Epoch 70 | Train: loss=0.291 acc=89.33% | Test: loss=0.149 acc=97.56% | time: 0m 24s | best-val-loss: 0.137 (epoch 58)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 71 | Train: loss=0.292 acc=89.34% | Test: loss=0.138 acc=96.24% | time: 0m 24s | best-val-loss: 0.137 (epoch 58)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 72 | Train: loss=0.285 acc=89.52% | Test: loss=0.186 acc=95.79% | time: 0m 23s | best-val-loss: 0.137 (epoch 58)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 73 | Train: loss=0.290 acc=89.34% | Test: loss=0.203 acc=95.12% | time: 0m 24s | best-val-loss: 0.137 (epoch 58)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 74 | Train: loss=0.282 acc=89.56% | Test: loss=0.176 acc=95.35% | time: 0m 24s | best-val-loss: 0.137 (epoch 58)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 75 | Train: loss=0.281 acc=89.62% | Test: loss=0.117 acc=97.12% | time: 0m 23s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 76 | Train: loss=0.281 acc=89.76% | Test: loss=0.123 acc=96.67% | time: 0m 24s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 77 | Train: loss=0.280 acc=89.75% | Test: loss=0.200 acc=95.35% | time: 0m 23s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 78 | Train: loss=0.276 acc=89.74% | Test: loss=0.149 acc=95.35% | time: 0m 24s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 79 | Train: loss=0.277 acc=89.81% | Test: loss=0.158 acc=96.89% | time: 0m 24s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 80 | Train: loss=0.273 acc=90.00% | Test: loss=0.125 acc=96.67% | time: 0m 24s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 81 | Train: loss=0.275 acc=89.92% | Test: loss=0.217 acc=95.35% | time: 0m 23s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 82 | Train: loss=0.271 acc=89.94% | Test: loss=0.177 acc=96.69% | time: 0m 24s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 83 | Train: loss=0.267 acc=90.10% | Test: loss=0.226 acc=94.25% | time: 0m 24s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 84 | Train: loss=0.267 acc=90.24% | Test: loss=0.198 acc=96.24% | time: 0m 23s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 85 | Train: loss=0.262 acc=90.29% | Test: loss=0.228 acc=94.69% | time: 0m 23s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 86 | Train: loss=0.268 acc=90.18% | Test: loss=0.218 acc=93.80% | time: 0m 24s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 87 | Train: loss=0.265 acc=90.30% | Test: loss=0.241 acc=94.47% | time: 0m 24s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 88 | Train: loss=0.261 acc=90.32% | Test: loss=0.154 acc=96.22% | time: 0m 23s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 89 | Train: loss=0.262 acc=90.29% | Test: loss=0.183 acc=96.24% | time: 0m 24s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 90 | Train: loss=0.260 acc=90.38% | Test: loss=0.222 acc=94.47% | time: 0m 23s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 91 | Train: loss=0.260 acc=90.35% | Test: loss=0.162 acc=96.45% | time: 0m 24s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 92 | Train: loss=0.257 acc=90.56% | Test: loss=0.242 acc=93.80% | time: 0m 24s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 93 | Train: loss=0.258 acc=90.55% | Test: loss=0.169 acc=95.57% | time: 0m 24s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 94 | Train: loss=0.253 acc=90.72% | Test: loss=0.171 acc=94.69% | time: 0m 24s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 95 | Train: loss=0.255 acc=90.61% | Test: loss=0.161 acc=96.89% | time: 0m 23s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 96 | Train: loss=0.251 acc=90.64% | Test: loss=0.167 acc=95.35% | time: 0m 23s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 97 | Train: loss=0.256 acc=90.46% | Test: loss=0.179 acc=95.35% | time: 0m 23s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 98 | Train: loss=0.255 acc=90.62% | Test: loss=0.203 acc=96.02% | time: 0m 24s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 99 | Train: loss=0.252 acc=90.67% | Test: loss=0.196 acc=95.35% | time: 0m 24s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 100 | Train: loss=0.249 acc=90.74% | Test: loss=0.169 acc=96.02% | time: 0m 24s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 101 | Train: loss=0.245 acc=90.88% | Test: loss=0.176 acc=96.02% | time: 0m 24s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 102 | Train: loss=0.248 acc=90.82% | Test: loss=0.217 acc=95.35% | time: 0m 23s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 103 | Train: loss=0.246 acc=90.90% | Test: loss=0.197 acc=95.35% | time: 0m 24s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 104 | Train: loss=0.247 acc=90.78% | Test: loss=0.217 acc=94.47% | time: 0m 24s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 105 | Train: loss=0.247 acc=90.83% | Test: loss=0.174 acc=95.79% | time: 0m 24s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 106 | Train: loss=0.247 acc=90.95% | Test: loss=0.136 acc=97.34% | time: 0m 23s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 107 | Train: loss=0.241 acc=91.06% | Test: loss=0.149 acc=96.22% | time: 0m 24s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 108 | Train: loss=0.244 acc=90.97% | Test: loss=0.155 acc=96.67% | time: 0m 24s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 109 | Train: loss=0.241 acc=91.10% | Test: loss=0.169 acc=97.12% | time: 0m 24s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 110 | Train: loss=0.240 acc=91.08% | Test: loss=0.176 acc=96.67% | time: 0m 24s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 111 | Train: loss=0.241 acc=91.05% | Test: loss=0.195 acc=95.35% | time: 0m 23s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 112 | Train: loss=0.237 acc=91.12% | Test: loss=0.182 acc=96.67% | time: 0m 24s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 113 | Train: loss=0.241 acc=91.03% | Test: loss=0.227 acc=95.57% | time: 0m 23s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 114 | Train: loss=0.243 acc=91.00% | Test: loss=0.174 acc=97.12% | time: 0m 24s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 115 | Train: loss=0.235 acc=91.27% | Test: loss=0.160 acc=96.02% | time: 0m 23s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 116 | Train: loss=0.238 acc=91.21% | Test: loss=0.156 acc=96.02% | time: 0m 24s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 117 | Train: loss=0.236 acc=91.25% | Test: loss=0.193 acc=96.22% | time: 0m 23s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 118 | Train: loss=0.235 acc=91.23% | Test: loss=0.189 acc=95.35% | time: 0m 23s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 119 | Train: loss=0.234 acc=91.29% | Test: loss=0.219 acc=95.12% | time: 0m 24s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 120 | Train: loss=0.235 acc=91.30% | Test: loss=0.220 acc=95.35% | time: 0m 23s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 121 | Train: loss=0.235 acc=91.25% | Test: loss=0.151 acc=96.89% | time: 0m 24s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 122 | Train: loss=0.234 acc=91.25% | Test: loss=0.176 acc=95.35% | time: 0m 23s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 123 | Train: loss=0.228 acc=91.47% | Test: loss=0.158 acc=97.34% | time: 0m 24s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 124 | Train: loss=0.231 acc=91.47% | Test: loss=0.179 acc=96.67% | time: 0m 24s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n",
      "Epoch 125 | Train: loss=0.228 acc=91.61% | Test: loss=0.219 acc=95.79% | time: 0m 23s | best-val-loss: 0.117 (epoch 75)| best-val-acc: 97.56% (epoch 70)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext import data, datasets\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define the LSTM-based model class\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, \n",
    "                 n_layers, bidirectional, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout if n_layers > 1 else 0.0)\n",
    "        # Linear layer input dimension depends on bidirectionality\n",
    "        fc_input_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        self.fc = nn.Linear(fc_input_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "    def forward(self, text, text_lengths):\n",
    "        # text: [sentence_length, batch_size]; text_lengths: [batch_size]\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        # Pack the sequence of embeddings\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'))\n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        # Unpack (pad) the sequence (output not used further here)\n",
    "        nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "        # hidden shape: [n_layers * num_directions, batch_size, hidden_dim]\n",
    "        if self.bidirectional:\n",
    "            # Concatenate the final forward and backward hidden states\n",
    "            hidden_combined = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        else:\n",
    "            # Use the final hidden state (from the last layer)\n",
    "            hidden_combined = hidden[-1,:,:]\n",
    "        # Apply dropout to final hidden state and pass through the linear layer\n",
    "        hidden_dropped = self.dropout(hidden_combined)\n",
    "        return self.fc(hidden_dropped)\n",
    "\n",
    "# Accuracy calculation\n",
    "def multiclass_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    preds: raw logits of shape [batch, n_classes]\n",
    "    y:     ground-truth indices, shape [batch]\n",
    "    \"\"\"\n",
    "    predicted_classes = preds.argmax(dim=1)\n",
    "    correct = (predicted_classes == y).float()\n",
    "    return correct.sum() / len(correct)\n",
    "\n",
    "# Training and evaluation functions\n",
    "def train_epoch(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        text, text_lengths = batch.text\n",
    "        predictions = model(text, text_lengths)\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        acc  = multiclass_accuracy(predictions, batch.label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate_epoch(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, text_lengths = batch.text\n",
    "            predictions = model(text, text_lengths)\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            acc = multiclass_accuracy(predictions, batch.label)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed = end_time - start_time\n",
    "    mins = int(elapsed // 60)\n",
    "    secs = int(elapsed % 60)\n",
    "    return mins, secs\n",
    "\n",
    "# Hyperparameter options\n",
    "# layer_options = [1,2,3,4,5]\n",
    "# bidir_options = [True]  \n",
    "# embedding_dims = [900,2700,4500]\n",
    "# dropout_options = [0.5,0.6,0.7]\n",
    "\n",
    "layer_options = [2]\n",
    "bidir_options = [True]  \n",
    "embedding_dims = [4500]\n",
    "dropout_options = [0.7]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "N_EPOCHS = 125\n",
    "HIDDEN_DIM = 256\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "# Loop over hyperparameter combinations\n",
    "for n_layers in layer_options:\n",
    "    for bidirectional in bidir_options:\n",
    "        for emb_dim in embedding_dims:\n",
    "            for dropout in dropout_options:\n",
    "                # Create fresh iterators for this configuration\n",
    "                train_iter, test_iter = data.BucketIterator.splits(\n",
    "                    (train_data, test_data),\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    sort_within_batch=True,\n",
    "                    device=device\n",
    "                )\n",
    "                # Initialize model with given hyperparameters\n",
    "                INPUT_DIM = len(TEXT.vocab)\n",
    "                model = LSTMClassifier(INPUT_DIM, emb_dim, HIDDEN_DIM, 4, \n",
    "                                       n_layers, bidirectional, dropout, \n",
    "                                       pad_idx=TEXT.vocab.stoi[TEXT.pad_token])\n",
    "                model = model.to(device)\n",
    "                # Zero-initialize the <unk> and <pad> token embeddings\n",
    "                UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "                PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "                model.embedding.weight.data[UNK_IDX] = torch.zeros(emb_dim)\n",
    "                model.embedding.weight.data[PAD_IDX] = torch.zeros(emb_dim)\n",
    "                optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "                \n",
    "                # Set up directory and logging for this run\n",
    "                run_dir = f\"./lstm_traget2/LSTM_layers{n_layers}_{'bidir' if bidirectional else 'uni'}_emb{emb_dim}_drop{dropout}\"\n",
    "                os.makedirs(run_dir, exist_ok=True)\n",
    "                metrics_path = os.path.join(run_dir, \"metrics.csv\")\n",
    "                with open(metrics_path, \"w\") as f:\n",
    "                    f.write(\"Epoch,TrainLoss,TrainAcc,TestLoss,TestAcc,Min,Sec\\n\")\n",
    "                \n",
    "                # Initialize trackers for best validation loss and accuracy\n",
    "                # best_valid_loss = float('inf')\n",
    "                # best_valid_acc = float('-inf')\n",
    "                best_test_loss = float('inf')\n",
    "                best_test_acc = float('-inf')\n",
    "\n",
    "                best_loss_epoch = None\n",
    "                best_acc_epoch = None\n",
    "                best_loss_path = os.path.join(run_dir, \"best_loss.pt\")\n",
    "                best_acc_path = os.path.join(run_dir, \"best_acc.pt\")\n",
    "                # Variables to store metrics at best epochs\n",
    "                best_loss_train_loss = best_loss_train_acc = None\n",
    "                best_loss_val_loss = best_loss_val_acc = None\n",
    "                best_loss_test_loss = best_loss_test_acc = None\n",
    "                best_acc_train_loss = best_acc_train_acc = None\n",
    "                best_acc_val_loss = best_acc_val_acc = None\n",
    "                best_acc_test_loss = best_acc_test_acc = None\n",
    "\n",
    "                print(f\"\\nTraining model | layers={n_layers}, bidirectional={bidirectional}, emb_dim={emb_dim}, dropout={dropout}\")\n",
    "                # Train for N_EPOCHS epochs\n",
    "                for epoch in range(1, N_EPOCHS + 1):\n",
    "                    start_time = time.time()\n",
    "                    train_loss, train_acc = train_epoch(model, train_iter, optimizer, criterion)\n",
    "                    # valid_loss, valid_acc = evaluate_epoch(model, valid_iter, criterion)\n",
    "                    test_loss, test_acc = evaluate_epoch(model, test_iter, criterion)\n",
    "                    end_time = time.time()\n",
    "                    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "                    \n",
    "                    # Check and save model for best validation loss\n",
    "                    if test_loss < best_test_loss:\n",
    "                        best_test_loss = test_loss\n",
    "                        best_loss_epoch = epoch\n",
    "                        torch.save(model.state_dict(), best_loss_path)\n",
    "                        # Record metrics at this epoch for best loss\n",
    "                        best_loss_train_loss = train_loss\n",
    "                        best_loss_train_acc = train_acc\n",
    "\n",
    "                        best_loss_test_loss = test_loss\n",
    "                        best_loss_test_acc = test_acc\n",
    "                    \n",
    "                    # Check and save model for best validation accuracy\n",
    "                    if test_acc > best_test_acc:\n",
    "                        best_test_acc = test_acc\n",
    "                        best_acc_epoch = epoch\n",
    "                        torch.save(model.state_dict(), best_acc_path)\n",
    "                        # Record metrics at this epoch for best accuracy\n",
    "                        best_acc_train_loss = train_loss\n",
    "                        best_acc_train_acc = train_acc\n",
    "\n",
    "                        best_acc_test_loss = test_loss\n",
    "                        best_acc_test_acc = test_acc\n",
    "\n",
    "                    # Log metrics for this epoch to CSV\n",
    "                    with open(metrics_path, \"a\") as f:\n",
    "                        f.write(f\"{epoch},{train_loss:.4f},{train_acc*100:.2f}%,\" +\n",
    "                                # f\"{valid_loss:.4f},{valid_acc*100:.2f}%,\" +\n",
    "                                f\"{test_loss:.4f},{test_acc*100:.2f}%,\" +\n",
    "                                f\"{epoch_mins},{epoch_secs}\\n\")\n",
    "                    # Print epoch summary\n",
    "                    print(\n",
    "                        f\"Epoch {epoch:02} | \"\n",
    "                        f\"Train: loss={train_loss:.3f} acc={train_acc*100:.2f}% | \"\n",
    "                        # f\"Val: loss={valid_loss:.3f} acc={valid_acc*100:.2f}% | \"\n",
    "                        f\"Test: loss={test_loss:.3f} acc={test_acc*100:.2f}% | \"\n",
    "                        f\"time: {epoch_mins}m {epoch_secs}s | \"\n",
    "                        f\"best-val-loss: {best_test_loss:.3f} (epoch {best_loss_epoch})| best-val-acc: {best_test_acc*100:.2f}% (epoch {best_acc_epoch})\"\n",
    "                    )\n",
    "\n",
    "                # After training, log the best epoch info for loss and accuracy\n",
    "                with open(metrics_path, \"a\") as f:\n",
    "                    f.write(\"\\n\")\n",
    "                    f.write(f\"BestLoss at epoch {best_loss_epoch}: \" +\n",
    "                            f\"TrainLoss={best_loss_train_loss:.4f}, TrainAcc={best_loss_train_acc*100:.2f}%, \" +\n",
    "                            # f\"ValidLoss={best_loss_val_loss:.4f}, ValidAcc={best_loss_val_acc*100:.2f}%, \" +\n",
    "                            f\"TestLoss={best_loss_test_loss:.4f}, TestAcc={best_loss_test_acc*100:.2f}%\\n\")\n",
    "                    f.write(f\"BestAcc at epoch {best_acc_epoch}: \" +\n",
    "                            f\"TrainLoss={best_acc_train_loss:.4f}, TrainAcc={best_acc_train_acc*100:.2f}%, \" +\n",
    "                            # f\"ValidLoss={best_acc_val_loss:.4f}, ValidAcc={best_acc_val_acc*100:.2f}%, \" +\n",
    "                            f\"TestLoss={best_acc_test_loss:.4f}, TestAcc={best_acc_test_acc*100:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c54aa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(TEXT, \"TEXT_field.pth\")\n",
    "torch.save(LABEL, \"LABEL_field.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f0a4d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: 2\n"
     ]
    }
   ],
   "source": [
    "# 1. Pick a device once\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 2. Move the model\n",
    "model = model.to(device)\n",
    "model.eval()                               # good habit for inference\n",
    "\n",
    "# 3. Build your example and move it\n",
    "example_sentence = \"This is a sample news headline to classify.\"\n",
    "\n",
    "tokens   = [tok.lower() for tok in TEXT.tokenize(example_sentence)]\n",
    "indices  = [TEXT.vocab.stoi[t] if t in TEXT.vocab.stoi\n",
    "            else TEXT.vocab.stoi[TEXT.unk_token] for t in tokens]\n",
    "\n",
    "text_tensor = torch.LongTensor(indices).unsqueeze(1).to(device)  # [seq_len, batch=1]\n",
    "text_length = torch.LongTensor([len(indices)]).to(device)        # [batch=1]\n",
    "\n",
    "# 4. Forward pass\n",
    "with torch.no_grad():\n",
    "    output = model(text_tensor, text_length)\n",
    "    prediction = output.argmax(dim=1).item()\n",
    "\n",
    "predicted_label = LABEL.vocab.itos[prediction]\n",
    "\n",
    "print(f\"Predicted label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f66cefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext import data, datasets\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define the LSTM-based model class\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, \n",
    "                 n_layers, bidirectional, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout if n_layers > 1 else 0.0)\n",
    "        # Linear layer input dimension depends on bidirectionality\n",
    "        fc_input_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        self.fc = nn.Linear(fc_input_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "    def forward(self, text, text_lengths):\n",
    "        # text: [sentence_length, batch_size]; text_lengths: [batch_size]\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        # Pack the sequence of embeddings\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'))\n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        # Unpack (pad) the sequence (output not used further here)\n",
    "        nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "        # hidden shape: [n_layers * num_directions, batch_size, hidden_dim]\n",
    "        if self.bidirectional:\n",
    "            # Concatenate the final forward and backward hidden states\n",
    "            hidden_combined = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        else:\n",
    "            # Use the final hidden state (from the last layer)\n",
    "            hidden_combined = hidden[-1,:,:]\n",
    "        # Apply dropout to final hidden state and pass through the linear layer\n",
    "        hidden_dropped = self.dropout(hidden_combined)\n",
    "        return self.fc(hidden_dropped)\n",
    "\n",
    "# Accuracy calculation\n",
    "def multiclass_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    preds: raw logits of shape [batch, n_classes]\n",
    "    y:     ground-truth indices, shape [batch]\n",
    "    \"\"\"\n",
    "    predicted_classes = preds.argmax(dim=1)\n",
    "    correct = (predicted_classes == y).float()\n",
    "    return correct.sum() / len(correct)\n",
    "\n",
    "# Training and evaluation functions\n",
    "def train_epoch(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        text, text_lengths = batch.text\n",
    "        predictions = model(text, text_lengths)\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        acc  = multiclass_accuracy(predictions, batch.label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate_epoch(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, text_lengths = batch.text\n",
    "            predictions = model(text, text_lengths)\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            acc = multiclass_accuracy(predictions, batch.label)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed = end_time - start_time\n",
    "    mins = int(elapsed // 60)\n",
    "    secs = int(elapsed % 60)\n",
    "    return mins, secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd0556a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMClassifier(\n",
       "  (embedding): Embedding(33270, 4500, padding_idx=1)\n",
       "  (rnn): LSTM(4500, 256, num_layers=3, dropout=0.7, bidirectional=True)\n",
       "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
       "  (dropout): Dropout(p=0.7, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load the saved fields\n",
    "# Load the saved fields with unpickling allowed\n",
    "TEXT = torch.load(\"TEXT_field.pth\", weights_only=False)\n",
    "LABEL = torch.load(\"LABEL_field.pth\", weights_only=False)\n",
    "\n",
    "# Recreate the model with same parameters as before\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 4500    # must match the embedding dim used in training\n",
    "HIDDEN_DIM = 256        # same hidden dim as training\n",
    "OUTPUT_DIM = len(LABEL.vocab)  # number of classes\n",
    "N_LAYERS = 3            # same as before\n",
    "BIDIRECTIONAL = True    # same as before\n",
    "DROPOUT = 0.7           # same as before\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = LSTMClassifier(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, \n",
    "                       N_LAYERS, BIDIRECTIONAL, DROPOUT, pad_idx=PAD_IDX)\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(\"best_acc.pt\", map_location=torch.device('cpu')))\n",
    "model.eval()  # set to evaluation mode\n",
    "# 1. Pick a device once\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43673f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Choose a device once\n",
    "# ------------------------------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using:\", device)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Reload the torch-text Fields\n",
    "# ------------------------------------------------------------\n",
    "TEXT  = torch.load(\"TEXT_field.pth\",  weights_only=False)\n",
    "LABEL = torch.load(\"LABEL_field.pth\", weights_only=False)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Re-create the model\n",
    "# ------------------------------------------------------------\n",
    "INPUT_DIM     = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 4500\n",
    "HIDDEN_DIM    = 256\n",
    "OUTPUT_DIM    = len(LABEL.vocab)\n",
    "N_LAYERS      = 3\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT       = 0.7\n",
    "PAD_IDX       = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = LSTMClassifier(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM,\n",
    "                       OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL,\n",
    "                       DROPOUT, pad_idx=PAD_IDX)\n",
    "\n",
    "# load weights, then move the whole model to the chosen device\n",
    "model.load_state_dict(torch.load(\"best_acc.pt\", map_location=device))\n",
    "model = model.to(device).eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3bf29c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3. Build your example and move it\n",
    "example_sentence = \"Great i love it \"\n",
    "\n",
    "tokens   = [tok.lower() for tok in TEXT.tokenize(example_sentence)]\n",
    "indices  = [TEXT.vocab.stoi[t] if t in TEXT.vocab.stoi\n",
    "            else TEXT.vocab.stoi[TEXT.unk_token] for t in tokens]\n",
    "\n",
    "text_tensor = torch.LongTensor(indices).unsqueeze(1).to(device)  # [seq_len, batch=1]\n",
    "text_length = torch.LongTensor([len(indices)]).to(device)        # [batch=1]\n",
    "\n",
    "# 4. Forward pass\n",
    "with torch.no_grad():\n",
    "    output = model(text_tensor, text_length)\n",
    "    prediction = output.argmax(dim=1).item()\n",
    "\n",
    "predicted_label = LABEL.vocab.itos[prediction]\n",
    "\n",
    "print(f\"Predicted label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "886e74e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    TrainLoss  TrainAcc  TestLoss  TestAcc  \\\n",
      "layers bidir emb_dim dropout Epoch                                           \n",
      "1      True  900     0.5     1         1.1087    0.5260    0.8557   0.6708   \n",
      "                             2         0.8257    0.6695    0.6050   0.8019   \n",
      "                             3         0.6443    0.7487    0.4518   0.8755   \n",
      "                             4         0.5273    0.7990    0.3843   0.8887   \n",
      "                             5         0.4543    0.8278    0.3128   0.8954   \n",
      "\n",
      "                                    Min   Sec  \n",
      "layers bidir emb_dim dropout Epoch             \n",
      "1      True  900     0.5     1        0  13.0  \n",
      "                             2        0  13.0  \n",
      "                             3        0  13.0  \n",
      "                             4        0   8.0  \n",
      "                             5        0   5.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "root = Path(\"./lstm_traget2\")\n",
    "csv_files = root.rglob(\"*.csv\")  # all CSVs underneath ./lstm\n",
    "\n",
    "# Parent-folder name looks like: LSTM_layers{N}_{bidir|uni}_emb{D}_drop{P}\n",
    "pat = re.compile(r\"LSTM_layers(\\d+)_(bidir|uni)_emb(\\d+)_drop([0-9.]+)\")\n",
    "\n",
    "# --- generic helpers ----------------------------------------------------------\n",
    "def _after_equals(x: str) -> str:\n",
    "    \"\"\"Take the substring after '=' (if '=' is present) and remove leading/trailing blanks.\"\"\"\n",
    "    return str(x).split(\"=\", 1)[-1].strip()\n",
    "\n",
    "def pct_to_float(x):\n",
    "    return float(_after_equals(x).rstrip(\"%\")) / 100\n",
    "\n",
    "def num_to_float(x):\n",
    "    return float(_after_equals(x))\n",
    "\n",
    "def num_to_int(x):\n",
    "    return int(_after_equals(x))\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "from pandas.errors import EmptyDataError\n",
    "\n",
    "def read_and_annotate(csv_path: Path) -> pd.DataFrame | None:\n",
    "    try:\n",
    "        df = pd.read_csv(\n",
    "            csv_path,\n",
    "            skipinitialspace=True,\n",
    "            comment=\"B\",\n",
    "            converters={\n",
    "                \"TrainLoss\": num_to_float,\n",
    "                \"TestLoss\":  num_to_float,\n",
    "                \"TrainAcc\":  pct_to_float,\n",
    "                \"TestAcc\":   pct_to_float,\n",
    "                \"Epoch\":     num_to_int,\n",
    "                \"Min\":       num_to_int,\n",
    "                \"Sec\":       num_to_float,\n",
    "            },\n",
    "        )\n",
    "    except EmptyDataError:\n",
    "        # nothing but comment/blank lines – silently ignore or log\n",
    "        print(f\"[skip] {csv_path} has no data rows.\")\n",
    "        return None\n",
    "\n",
    "    if df.empty:\n",
    "        print(f\"[skip] {csv_path} became empty after parsing.\")\n",
    "        return None\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # annotate with hyper-parameters\n",
    "    m = pat.fullmatch(csv_path.parent.name)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Folder name {csv_path.parent.name} does not match pattern\")\n",
    "    n_layers, direction, emb_dim, dropout = m.groups()\n",
    "\n",
    "    df[\"layers\"]  = int(n_layers)\n",
    "    df[\"bidir\"]   = (direction == \"bidir\")\n",
    "    df[\"emb_dim\"] = int(emb_dim)\n",
    "    df[\"dropout\"] = float(dropout)\n",
    "    # --------------------------------------------------------------\n",
    "    return df\n",
    "\n",
    "\n",
    "# build the list and drop the Nones\n",
    "frames = [f for f in (read_and_annotate(p) for p in root.rglob(\"*.csv\")) if f is not None]\n",
    "\n",
    "df_all = (\n",
    "    pd.concat(frames, ignore_index=True)\n",
    "      .set_index([\"layers\", \"bidir\", \"emb_dim\", \"dropout\", \"Epoch\"])\n",
    "      .sort_index()\n",
    ")\n",
    "\n",
    "# Build the big tidy DataFrame by concatenating all annotated DataFrames\n",
    "df_all = pd.concat([read_and_annotate(p) for p in csv_files], ignore_index=True)\n",
    "\n",
    "# (Optional) set a multi-index for easier slicing of hyper-parameters and sort it\n",
    "df_all = df_all.set_index([\"layers\", \"bidir\", \"emb_dim\", \"dropout\", \"Epoch\"]).sort_index()\n",
    "\n",
    "print(df_all.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4dbc1bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index to get columns back (if you set a multi-index before)\n",
    "df_all_reset = df_all.reset_index()\n",
    "\n",
    "# Select only the columns of interest: hyperparameters, Epoch, and validation/test metrics\n",
    "df_filtered = df_all_reset[[\n",
    "    \"layers\", \"bidir\", \"emb_dim\", \"dropout\", \"Epoch\",\"TrainLoss\",\"TrainAcc\",\n",
    "      \"TestLoss\", \"TestAcc\"\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b264c5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_options = [1,2]\n",
    "bidir_options = [True]  \n",
    "embedding_dims = [900,1800,2700,3600,4500]\n",
    "dropout_options = [0.5,0.6,0.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c845da6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " TrainAcc  TestAcc  TrainLoss  TestLoss  Epoch  layers  bidir  emb_dim  dropout\n",
      "   0.9486   0.9978     0.1351    0.0341     32       2   True     2700      0.5\n",
      "   0.9314   0.9955     0.1805    0.0350     21       2   True      900      0.5\n",
      "   0.9427   0.9955     0.1488    0.0331     24       2   True     4500      0.5\n",
      "   0.9379   0.9955     0.1635    0.0375     72       2   True      900      0.6\n",
      "   0.8993   0.9933     0.2716    0.0436     21       2   True     4500      0.6\n",
      "   0.9658   0.9933     0.0883    0.0850    109       1   True      900      0.5\n",
      "   0.9431   0.9911     0.1520    0.0403     79       2   True     2700      0.6\n",
      "   0.9647   0.9888     0.0926    0.1389     88       1   True     2700      0.5\n",
      "   0.9487   0.9888     0.1346    0.1042    103       1   True      900      0.6\n",
      "   0.9682   0.9888     0.0861    0.1663    125       1   True     4500      0.5\n",
      "   0.8859   0.9866     0.3086    0.0828     64       3   True     4500      0.7\n",
      "   0.9109   0.9866     0.2366    0.0614    108       2   True      900      0.7\n",
      "   0.9441   0.9844     0.1512    0.1264     94       1   True     4500      0.6\n",
      "   0.9158   0.9801     0.2266    0.1260    125       2   True     2700      0.7\n",
      "   0.9297   0.9799     0.1903    0.1082     51       1   True     2700      0.6\n",
      "   0.8933   0.9756     0.2915    0.1492     70       2   True     4500      0.7\n",
      "   0.9225   0.9378     0.2068    0.2968    124       1   True      900      0.7\n",
      "   0.9011   0.9310     0.2663    0.3001    110       1   True     4500      0.7\n",
      "   0.9131   0.9263     0.2329    0.3028    109       1   True     2700      0.7\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 1. For every (layers, bidir, emb_dim, dropout) combo: row with max ValidAcc\n",
    "# ------------------------------------------------------------------\n",
    "best_valid_rows = (\n",
    "    df_filtered\n",
    "        .loc[                                  # keep the whole row …\n",
    "            df_filtered                         # … that has the index\n",
    "            .groupby([\"layers\", \"bidir\", \"emb_dim\", \"dropout\"])[\"TestAcc\"]\n",
    "            .idxmax()                           # of the epoch with max ValidAcc\n",
    "        ]\n",
    "        .sort_values(\"TestAcc\", ascending=False)   # 2. sort descending\n",
    "        .reset_index(drop=True)                     # tidy new index (optional)\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2. Show / export the ranking\n",
    "# ------------------------------------------------------------------\n",
    "cols_to_show = [\n",
    "    \"TrainAcc\", \"TestAcc\",          # main numbers of interest\n",
    "    \"TrainLoss\", \"TestLoss\",        # maybe useful too\n",
    "    \"Epoch\",                        # epoch where the max was reached\n",
    "    \"layers\", \"bidir\", \"emb_dim\", \"dropout\"  # hyper-parameters\n",
    "]\n",
    "\n",
    "print(best_valid_rows[cols_to_show].to_string(index=False))\n",
    "\n",
    "# If you prefer to save it:\n",
    "# best_valid_rows[cols_to_show].to_csv(\"best_by_valid_acc.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3084a4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " layers  bidir  emb_dim  dropout\n",
      "      2   True     3600      0.7\n",
      "      4   True     1800      0.7\n",
      "      4   True     3600      0.7\n",
      "      2   True     2700      0.7\n",
      "      3   True     4500      0.7\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Columns that define one hyper-parameter configuration\n",
    "# -------------------------------------------------------------\n",
    "param_cols = [\"layers\", \"bidir\", \"emb_dim\", \"dropout\"]\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 1) keep rows with TestAcc > 0.46\n",
    "# 2) keep only the hyper-parameter columns\n",
    "# 3) drop duplicate configurations\n",
    "# 4) reset the index (optional, just for neat printing / saving)\n",
    "# -------------------------------------------------------------\n",
    "good_param_sets = (\n",
    "    best_valid_rows                      # or df_filtered, if you prefer\n",
    "        .loc[best_valid_rows[\"TestAcc\"] > 0.46, param_cols]\n",
    "        .drop_duplicates()\n",
    "        .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(good_param_sets.to_string(index=False))\n",
    "\n",
    "# If you want to save it:\n",
    "# good_param_sets.to_csv(\"good_param_sets.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "490f4bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " MeanTestAcc  EpochCnt  FirstEpoch  LastEpoch  layers  bidir  emb_dim  dropout\n",
      "    0.439950        50          76        125       4   True     3600      0.7\n",
      "    0.434474        50          76        125       4   True     1800      0.7\n",
      "    0.426802        50          76        125       2   True     2700      0.7\n",
      "    0.425330        50          76        125       2   True     4500      0.7\n",
      "    0.422006        50          76        125       2   True     1800      0.7\n",
      "    0.421928        50          76        125       3   True     4500      0.7\n",
      "    0.421052        50          76        125       4   True     2700      0.7\n",
      "    0.420760        50          76        125       2   True     3600      0.7\n",
      "    0.414538        50          76        125       3   True     2700      0.7\n",
      "    0.404500        50          76        125       4   True     4500      0.7\n",
      "    0.403466        50          76        125       3   True     3600      0.7\n",
      "    0.401646        50          76        125       3   True     1800      0.7\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# 0. Hyper-parameter grid we are interested in\n",
    "# ---------------------------------------------------------------\n",
    "layer_options   = [2, 3, 4]\n",
    "bidir_options   = [True]\n",
    "embedding_dims  = [1800, 2700, 3600, 4500]\n",
    "dropout_options = [0.7]\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 1. Keep only the runs that belong to that grid\n",
    "# ---------------------------------------------------------------\n",
    "mask = (\n",
    "    df_filtered[\"layers\"].isin(layer_options)   &\n",
    "    df_filtered[\"bidir\"].isin(bidir_options)    &\n",
    "    df_filtered[\"emb_dim\"].isin(embedding_dims) &\n",
    "    df_filtered[\"dropout\"].isin(dropout_options)\n",
    ")\n",
    "df_sub = df_filtered.loc[mask]\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 2. Per run → retain the LAST 50 epochs (or fewer if run is short)\n",
    "# ---------------------------------------------------------------\n",
    "df_last50 = (\n",
    "    df_sub\n",
    "      .sort_values([\"layers\", \"bidir\", \"emb_dim\", \"dropout\", \"Epoch\"])\n",
    "      .groupby([\"layers\", \"bidir\", \"emb_dim\", \"dropout\"], as_index=False, group_keys=False)\n",
    "      .tail(50)                  # keep the 50 largest Epoch numbers of each group\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 3. Compute the mean TestAcc on those epochs\n",
    "# ---------------------------------------------------------------\n",
    "mean_test_acc = (\n",
    "    df_last50\n",
    "      .groupby([\"layers\", \"bidir\", \"emb_dim\", \"dropout\"], as_index=False)\n",
    "      .agg(MeanTestAcc=(\"TestAcc\", \"mean\"),\n",
    "           EpochCnt   =(\"Epoch\",   \"nunique\"),   # how many epochs actually used\n",
    "           FirstEpoch =(\"Epoch\",   \"min\"),       # first epoch kept (optional)\n",
    "           LastEpoch  =(\"Epoch\",   \"max\"))       # last  epoch kept (optional)\n",
    "      .sort_values(\"MeanTestAcc\", ascending=False)\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 4. Show / export\n",
    "# ---------------------------------------------------------------\n",
    "cols_to_show = [\"MeanTestAcc\", \"EpochCnt\", \"FirstEpoch\", \"LastEpoch\",\n",
    "                \"layers\", \"bidir\", \"emb_dim\", \"dropout\"]\n",
    "print(mean_test_acc[cols_to_show].to_string(index=False))\n",
    "\n",
    "# Optional: save to disk\n",
    "# mean_test_acc[cols_to_show].to_csv(\"mean_test_acc_last50.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a04728",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bd_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

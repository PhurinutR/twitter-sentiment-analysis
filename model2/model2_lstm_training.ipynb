{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99854bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fe34728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "TEXT = data.Field(tokenize = 'spacy',\n",
    "                  tokenizer_language = 'en_core_web_trf', # en_core_web_md , en_core_web_lg\n",
    "                  include_lengths = True)\n",
    "LABEL = data.LabelField(dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4f61c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Use torchtext.legacy.data if it exists, otherwise torchtext.data\n",
    "# ---------------------------------------------------------------\n",
    "try:                     # torchtext >= 0.9\n",
    "    from torchtext.legacy import data \n",
    "except ImportError:      # torchtext <= 0.8 (your 0.5.0 case)\n",
    "    from torchtext import data\n",
    "\n",
    "class FolderDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    root/\n",
    "      ├── train/\n",
    "      │     ├── classA/\n",
    "      │     │     ├── xxx.txt\n",
    "      │     │     └── yyy.txt\n",
    "      │     └── classB/\n",
    "      └── test/\n",
    "            └── ...\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def sort_key(ex):            # lets BucketIterator bucket by length\n",
    "        return len(ex.text)\n",
    "\n",
    "    def __init__(self, root, text_field, label_field,\n",
    "                 encoding=\"utf-8\", **kwargs):\n",
    "        fields   = [(\"text\", text_field), (\"label\", label_field)]\n",
    "        examples = []\n",
    "\n",
    "        root = pathlib.Path(root)\n",
    "        for class_dir in sorted(p for p in root.iterdir() if p.is_dir()):\n",
    "            label = class_dir.name                      # folder name → label\n",
    "            for fp in class_dir.glob(\"*.txt\"):\n",
    "                txt = fp.read_text(encoding=encoding)\n",
    "                examples.append(data.Example.fromlist([txt, label], fields))\n",
    "\n",
    "        super(FolderDataset, self).__init__(examples, fields, **kwargs)\n",
    "\n",
    "    # ------------ helper that mimics datasets.IMDB.splits ------------\n",
    "    @classmethod\n",
    "    def splits(cls, text_field, label_field,\n",
    "               path, train=\"train\", test=\"test\", **kwargs):\n",
    "        train_ds = cls(os.path.join(path, train),\n",
    "                       text_field, label_field, **kwargs)\n",
    "        test_ds  = cls(os.path.join(path, test),\n",
    "                       text_field, label_field, **kwargs)\n",
    "        return train_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa3e572d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train examples : 58907\n",
      "test  examples : 397\n"
     ]
    }
   ],
   "source": [
    "from torchtext import data            # 0.5 import\n",
    "\n",
    "# 2) Datasets ----------------------------------------------------------\n",
    "train_data, test_data = FolderDataset.splits(\n",
    "        TEXT, LABEL,\n",
    "        path=\"data\"          # <- folder that contains /train and /test\n",
    ")\n",
    "\n",
    "print(\"train examples :\", len(train_data))\n",
    "print(\"test  examples :\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ae65acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.840B.300d.zip: 2.18GB [07:05, 5.12MB/s]                                \n",
      "100%|█████████▉| 2196016/2196017 [01:15<00:00, 28983.67it/s]\n"
     ]
    }
   ],
   "source": [
    "MAX_VOCAB_SIZE = 250_000\n",
    "TEXT.build_vocab(train_data,\n",
    "                 max_size = MAX_VOCAB_SIZE,\n",
    "                 vectors = \"glove.840B.300d\",       #\"glove.6B.100d\",glove.840B.300d  glove.42B.300d  \"glove.twitter.27B.200d\"     # Do not use complicated word-embedding to reduce download time\n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "LABEL.build_vocab(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cc97838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 55874), ('the', 35601), ('be', 32888), ('i', 29555), (',', 25508), ('!', 23605), ('to', 22903), ('a', 21937), ('and', 21301), ('of', 15566), ('it', 15334), ('for', 12538), ('/', 12502), ('in', 12349), ('this', 11820), ('have', 10724), ('you', 10471), ('?', 10175), ('...', 9835), ('on', 9679)]\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.freqs.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58a0a0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1302628\n"
     ]
    }
   ],
   "source": [
    "total_tokens = sum(TEXT.vocab.freqs.values())\n",
    "print(total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ffbba6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, test_data),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_within_batch = True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0c5490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext import data, datasets\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define the LSTM-based model class\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, \n",
    "                 n_layers, bidirectional, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout if n_layers > 1 else 0.0)\n",
    "        # Linear layer input dimension depends on bidirectionality\n",
    "        fc_input_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        self.fc = nn.Linear(fc_input_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "    def forward(self, text, text_lengths):\n",
    "        # text: [sentence_length, batch_size]; text_lengths: [batch_size]\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        # Pack the sequence of embeddings\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'))\n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        # Unpack (pad) the sequence (output not used further here)\n",
    "        nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "        # hidden shape: [n_layers * num_directions, batch_size, hidden_dim]\n",
    "        if self.bidirectional:\n",
    "            # Concatenate the final forward and backward hidden states\n",
    "            hidden_combined = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        else:\n",
    "            # Use the final hidden state (from the last layer)\n",
    "            hidden_combined = hidden[-1,:,:]\n",
    "        # Apply dropout to final hidden state and pass through the linear layer\n",
    "        hidden_dropped = self.dropout(hidden_combined)\n",
    "        return self.fc(hidden_dropped)\n",
    "\n",
    "# Accuracy calculation\n",
    "def multiclass_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    preds: raw logits of shape [batch, n_classes]\n",
    "    y:     ground-truth indices, shape [batch]\n",
    "    \"\"\"\n",
    "    predicted_classes = preds.argmax(dim=1)\n",
    "    correct = (predicted_classes == y).float()\n",
    "    return correct.sum() / len(correct)\n",
    "\n",
    "# Training and evaluation functions\n",
    "def train_epoch(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        text, text_lengths = batch.text\n",
    "        predictions = model(text, text_lengths)\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        acc  = multiclass_accuracy(predictions, batch.label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate_epoch(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, text_lengths = batch.text\n",
    "            predictions = model(text, text_lengths)\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            acc = multiclass_accuracy(predictions, batch.label)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed = end_time - start_time\n",
    "    mins = int(elapsed // 60)\n",
    "    secs = int(elapsed % 60)\n",
    "    return mins, secs\n",
    "\n",
    "# Hyperparameter options\n",
    "layer_options = [1,2,3,4,5]\n",
    "bidir_options = [True]  \n",
    "embedding_dims = [900,2700,4500]\n",
    "dropout_options = [0.5,0.6,0.7]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "N_EPOCHS = 125\n",
    "HIDDEN_DIM = 256\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "# Loop over hyperparameter combinations\n",
    "for n_layers in layer_options:\n",
    "    for bidirectional in bidir_options:\n",
    "        for emb_dim in embedding_dims:\n",
    "            for dropout in dropout_options:\n",
    "                # Create fresh iterators for this configuration\n",
    "                train_iter, test_iter = data.BucketIterator.splits(\n",
    "                    (train_data, test_data),\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    sort_within_batch=True,\n",
    "                    device=device\n",
    "                )\n",
    "                # Initialize model with given hyperparameters\n",
    "                INPUT_DIM = len(TEXT.vocab)\n",
    "                model = LSTMClassifier(INPUT_DIM, emb_dim, HIDDEN_DIM, 4, \n",
    "                                       n_layers, bidirectional, dropout, \n",
    "                                       pad_idx=TEXT.vocab.stoi[TEXT.pad_token])\n",
    "                model = model.to(device)\n",
    "                # Zero-initialize the <unk> and <pad> token embeddings\n",
    "                UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "                PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "                model.embedding.weight.data[UNK_IDX] = torch.zeros(emb_dim)\n",
    "                model.embedding.weight.data[PAD_IDX] = torch.zeros(emb_dim)\n",
    "                optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "                \n",
    "                # Set up directory and logging for this run\n",
    "                run_dir = f\"./lstm_traget2/LSTM_layers{n_layers}_{'bidir' if bidirectional else 'uni'}_emb{emb_dim}_drop{dropout}\"\n",
    "                os.makedirs(run_dir, exist_ok=True)\n",
    "                metrics_path = os.path.join(run_dir, \"metrics.csv\")\n",
    "                with open(metrics_path, \"w\") as f:\n",
    "                    f.write(\"Epoch,TrainLoss,TrainAcc,TestLoss,TestAcc,Min,Sec\\n\")\n",
    "                \n",
    "                # Initialize trackers for best validation loss and accuracy\n",
    "                # best_valid_loss = float('inf')\n",
    "                # best_valid_acc = float('-inf')\n",
    "                best_test_loss = float('inf')\n",
    "                best_test_acc = float('-inf')\n",
    "\n",
    "                best_loss_epoch = None\n",
    "                best_acc_epoch = None\n",
    "                best_loss_path = os.path.join(run_dir, \"best_loss.pt\")\n",
    "                best_acc_path = os.path.join(run_dir, \"best_acc.pt\")\n",
    "                # Variables to store metrics at best epochs\n",
    "                best_loss_train_loss = best_loss_train_acc = None\n",
    "                best_loss_val_loss = best_loss_val_acc = None\n",
    "                best_loss_test_loss = best_loss_test_acc = None\n",
    "                best_acc_train_loss = best_acc_train_acc = None\n",
    "                best_acc_val_loss = best_acc_val_acc = None\n",
    "                best_acc_test_loss = best_acc_test_acc = None\n",
    "\n",
    "                print(f\"\\nTraining model | layers={n_layers}, bidirectional={bidirectional}, emb_dim={emb_dim}, dropout={dropout}\")\n",
    "                # Train for N_EPOCHS epochs\n",
    "                for epoch in range(1, N_EPOCHS + 1):\n",
    "                    start_time = time.time()\n",
    "                    train_loss, train_acc = train_epoch(model, train_iter, optimizer, criterion)\n",
    "                    # valid_loss, valid_acc = evaluate_epoch(model, valid_iter, criterion)\n",
    "                    test_loss, test_acc = evaluate_epoch(model, test_iter, criterion)\n",
    "                    end_time = time.time()\n",
    "                    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "                    \n",
    "                    # Check and save model for best validation loss\n",
    "                    if test_loss < best_test_loss:\n",
    "                        best_test_loss = test_loss\n",
    "                        best_loss_epoch = epoch\n",
    "                        torch.save(model.state_dict(), best_loss_path)\n",
    "                        # Record metrics at this epoch for best loss\n",
    "                        best_loss_train_loss = train_loss\n",
    "                        best_loss_train_acc = train_acc\n",
    "\n",
    "                        best_loss_test_loss = test_loss\n",
    "                        best_loss_test_acc = test_acc\n",
    "                    \n",
    "                    # Check and save model for best validation accuracy\n",
    "                    if test_acc > best_test_acc:\n",
    "                        best_test_acc = test_acc\n",
    "                        best_acc_epoch = epoch\n",
    "                        torch.save(model.state_dict(), best_acc_path)\n",
    "                        # Record metrics at this epoch for best accuracy\n",
    "                        best_acc_train_loss = train_loss\n",
    "                        best_acc_train_acc = train_acc\n",
    "\n",
    "                        best_acc_test_loss = test_loss\n",
    "                        best_acc_test_acc = test_acc\n",
    "\n",
    "                    # Log metrics for this epoch to CSV\n",
    "                    with open(metrics_path, \"a\") as f:\n",
    "                        f.write(f\"{epoch},{train_loss:.4f},{train_acc*100:.2f}%,\" +\n",
    "                                # f\"{valid_loss:.4f},{valid_acc*100:.2f}%,\" +\n",
    "                                f\"{test_loss:.4f},{test_acc*100:.2f}%,\" +\n",
    "                                f\"{epoch_mins},{epoch_secs}\\n\")\n",
    "                    # Print epoch summary\n",
    "                    print(\n",
    "                        f\"Epoch {epoch:02} | \"\n",
    "                        f\"Train: loss={train_loss:.3f} acc={train_acc*100:.2f}% | \"\n",
    "                        # f\"Val: loss={valid_loss:.3f} acc={valid_acc*100:.2f}% | \"\n",
    "                        f\"Test: loss={test_loss:.3f} acc={test_acc*100:.2f}% | \"\n",
    "                        f\"time: {epoch_mins}m {epoch_secs}s | \"\n",
    "                        f\"best-val-loss: {best_test_loss:.3f} (epoch {best_loss_epoch})| best-val-acc: {best_test_acc*100:.2f}% (epoch {best_acc_epoch})\"\n",
    "                    )\n",
    "\n",
    "                # After training, log the best epoch info for loss and accuracy\n",
    "                with open(metrics_path, \"a\") as f:\n",
    "                    f.write(\"\\n\")\n",
    "                    f.write(f\"BestLoss at epoch {best_loss_epoch}: \" +\n",
    "                            f\"TrainLoss={best_loss_train_loss:.4f}, TrainAcc={best_loss_train_acc*100:.2f}%, \" +\n",
    "                            # f\"ValidLoss={best_loss_val_loss:.4f}, ValidAcc={best_loss_val_acc*100:.2f}%, \" +\n",
    "                            f\"TestLoss={best_loss_test_loss:.4f}, TestAcc={best_loss_test_acc*100:.2f}%\\n\")\n",
    "                    f.write(f\"BestAcc at epoch {best_acc_epoch}: \" +\n",
    "                            f\"TrainLoss={best_acc_train_loss:.4f}, TrainAcc={best_acc_train_acc*100:.2f}%, \" +\n",
    "                            # f\"ValidLoss={best_acc_val_loss:.4f}, ValidAcc={best_acc_val_acc*100:.2f}%, \" +\n",
    "                            f\"TestLoss={best_acc_test_loss:.4f}, TestAcc={best_acc_test_acc*100:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c54aa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(TEXT, \"TEXT_field.pth\")\n",
    "torch.save(LABEL, \"LABEL_field.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f0a4d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: 2\n"
     ]
    }
   ],
   "source": [
    "# 1. Pick a device once\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 2. Move the model\n",
    "model = model.to(device)\n",
    "model.eval()                               # good habit for inference\n",
    "\n",
    "# 3. Build your example and move it\n",
    "example_sentence = \"This is a sample news headline to classify.\"\n",
    "\n",
    "tokens   = [tok.lower() for tok in TEXT.tokenize(example_sentence)]\n",
    "indices  = [TEXT.vocab.stoi[t] if t in TEXT.vocab.stoi\n",
    "            else TEXT.vocab.stoi[TEXT.unk_token] for t in tokens]\n",
    "\n",
    "text_tensor = torch.LongTensor(indices).unsqueeze(1).to(device)  # [seq_len, batch=1]\n",
    "text_length = torch.LongTensor([len(indices)]).to(device)        # [batch=1]\n",
    "\n",
    "# 4. Forward pass\n",
    "with torch.no_grad():\n",
    "    output = model(text_tensor, text_length)\n",
    "    prediction = output.argmax(dim=1).item()\n",
    "\n",
    "predicted_label = LABEL.vocab.itos[prediction]\n",
    "\n",
    "print(f\"Predicted label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f66cefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext import data, datasets\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define the LSTM-based model class\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, \n",
    "                 n_layers, bidirectional, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout if n_layers > 1 else 0.0)\n",
    "        # Linear layer input dimension depends on bidirectionality\n",
    "        fc_input_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        self.fc = nn.Linear(fc_input_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "    def forward(self, text, text_lengths):\n",
    "        # text: [sentence_length, batch_size]; text_lengths: [batch_size]\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        # Pack the sequence of embeddings\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'))\n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        # Unpack (pad) the sequence (output not used further here)\n",
    "        nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "        # hidden shape: [n_layers * num_directions, batch_size, hidden_dim]\n",
    "        if self.bidirectional:\n",
    "            # Concatenate the final forward and backward hidden states\n",
    "            hidden_combined = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        else:\n",
    "            # Use the final hidden state (from the last layer)\n",
    "            hidden_combined = hidden[-1,:,:]\n",
    "        # Apply dropout to final hidden state and pass through the linear layer\n",
    "        hidden_dropped = self.dropout(hidden_combined)\n",
    "        return self.fc(hidden_dropped)\n",
    "\n",
    "# Accuracy calculation\n",
    "def multiclass_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    preds: raw logits of shape [batch, n_classes]\n",
    "    y:     ground-truth indices, shape [batch]\n",
    "    \"\"\"\n",
    "    predicted_classes = preds.argmax(dim=1)\n",
    "    correct = (predicted_classes == y).float()\n",
    "    return correct.sum() / len(correct)\n",
    "\n",
    "# Training and evaluation functions\n",
    "def train_epoch(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        text, text_lengths = batch.text\n",
    "        predictions = model(text, text_lengths)\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        acc  = multiclass_accuracy(predictions, batch.label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate_epoch(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, text_lengths = batch.text\n",
    "            predictions = model(text, text_lengths)\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            acc = multiclass_accuracy(predictions, batch.label)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed = end_time - start_time\n",
    "    mins = int(elapsed // 60)\n",
    "    secs = int(elapsed % 60)\n",
    "    return mins, secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd0556a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMClassifier(\n",
       "  (embedding): Embedding(33270, 4500, padding_idx=1)\n",
       "  (rnn): LSTM(4500, 256, num_layers=3, dropout=0.7, bidirectional=True)\n",
       "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
       "  (dropout): Dropout(p=0.7, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load the saved fields\n",
    "# Load the saved fields with unpickling allowed\n",
    "TEXT = torch.load(\"TEXT_field.pth\", weights_only=False)\n",
    "LABEL = torch.load(\"LABEL_field.pth\", weights_only=False)\n",
    "\n",
    "# Recreate the model with same parameters as before\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 4500    # must match the embedding dim used in training\n",
    "HIDDEN_DIM = 256        # same hidden dim as training\n",
    "OUTPUT_DIM = len(LABEL.vocab)  # number of classes\n",
    "N_LAYERS = 3            # same as before\n",
    "BIDIRECTIONAL = True    # same as before\n",
    "DROPOUT = 0.7           # same as before\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = LSTMClassifier(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, \n",
    "                       N_LAYERS, BIDIRECTIONAL, DROPOUT, pad_idx=PAD_IDX)\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(\"best_acc.pt\", map_location=torch.device('cpu')))\n",
    "model.eval()  # set to evaluation mode\n",
    "# 1. Pick a device once\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43673f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Choose a device once\n",
    "# ------------------------------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using:\", device)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Reload the torch-text Fields\n",
    "# ------------------------------------------------------------\n",
    "TEXT  = torch.load(\"TEXT_field.pth\",  weights_only=False)\n",
    "LABEL = torch.load(\"LABEL_field.pth\", weights_only=False)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Re-create the model\n",
    "# ------------------------------------------------------------\n",
    "INPUT_DIM     = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 4500\n",
    "HIDDEN_DIM    = 256\n",
    "OUTPUT_DIM    = len(LABEL.vocab)\n",
    "N_LAYERS      = 3\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT       = 0.7\n",
    "PAD_IDX       = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = LSTMClassifier(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM,\n",
    "                       OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL,\n",
    "                       DROPOUT, pad_idx=PAD_IDX)\n",
    "\n",
    "# load weights, then move the whole model to the chosen device\n",
    "model.load_state_dict(torch.load(\"best_acc.pt\", map_location=device))\n",
    "model = model.to(device).eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3bf29c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3. Build your example and move it\n",
    "example_sentence = \"Great i love it \"\n",
    "\n",
    "tokens   = [tok.lower() for tok in TEXT.tokenize(example_sentence)]\n",
    "indices  = [TEXT.vocab.stoi[t] if t in TEXT.vocab.stoi\n",
    "            else TEXT.vocab.stoi[TEXT.unk_token] for t in tokens]\n",
    "\n",
    "text_tensor = torch.LongTensor(indices).unsqueeze(1).to(device)  # [seq_len, batch=1]\n",
    "text_length = torch.LongTensor([len(indices)]).to(device)        # [batch=1]\n",
    "\n",
    "# 4. Forward pass\n",
    "with torch.no_grad():\n",
    "    output = model(text_tensor, text_length)\n",
    "    prediction = output.argmax(dim=1).item()\n",
    "\n",
    "predicted_label = LABEL.vocab.itos[prediction]\n",
    "\n",
    "print(f\"Predicted label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "886e74e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    TrainLoss  TrainAcc  TestLoss  TestAcc  \\\n",
      "layers bidir emb_dim dropout Epoch                                           \n",
      "2      True  1800    0.7     1         1.3608    0.3154    1.3437   0.3479   \n",
      "                             2         1.2568    0.4485    1.3557   0.3877   \n",
      "                             3         1.0372    0.5691    1.8096   0.4281   \n",
      "                             4         0.8261    0.6886    1.7333   0.4013   \n",
      "                             5         0.5621    0.7928    2.1567   0.4033   \n",
      "\n",
      "                                    Min  Sec  \n",
      "layers bidir emb_dim dropout Epoch            \n",
      "2      True  1800    0.7     1        0  0.0  \n",
      "                             2        0  0.0  \n",
      "                             3        0  0.0  \n",
      "                             4        0  0.0  \n",
      "                             5        0  0.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "root = Path(\"./lstm_traget2\")\n",
    "csv_files = root.rglob(\"*.csv\")  # all CSVs underneath ./lstm\n",
    "\n",
    "# Parent-folder name looks like: LSTM_layers{N}_{bidir|uni}_emb{D}_drop{P}\n",
    "pat = re.compile(r\"LSTM_layers(\\d+)_(bidir|uni)_emb(\\d+)_drop([0-9.]+)\")\n",
    "\n",
    "# --- generic helpers ----------------------------------------------------------\n",
    "def _after_equals(x: str) -> str:\n",
    "    \"\"\"Take the substring after '=' (if '=' is present) and remove leading/trailing blanks.\"\"\"\n",
    "    return str(x).split(\"=\", 1)[-1].strip()\n",
    "\n",
    "def pct_to_float(x):\n",
    "    return float(_after_equals(x).rstrip(\"%\")) / 100\n",
    "\n",
    "def num_to_float(x):\n",
    "    return float(_after_equals(x))\n",
    "\n",
    "def num_to_int(x):\n",
    "    return int(_after_equals(x))\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "from pandas.errors import EmptyDataError\n",
    "\n",
    "def read_and_annotate(csv_path: Path) -> pd.DataFrame | None:\n",
    "    try:\n",
    "        df = pd.read_csv(\n",
    "            csv_path,\n",
    "            skipinitialspace=True,\n",
    "            comment=\"B\",\n",
    "            converters={\n",
    "                \"TrainLoss\": num_to_float,\n",
    "                \"TestLoss\":  num_to_float,\n",
    "                \"TrainAcc\":  pct_to_float,\n",
    "                \"TestAcc\":   pct_to_float,\n",
    "                \"Epoch\":     num_to_int,\n",
    "                \"Min\":       num_to_int,\n",
    "                \"Sec\":       num_to_float,\n",
    "            },\n",
    "        )\n",
    "    except EmptyDataError:\n",
    "        # nothing but comment/blank lines – silently ignore or log\n",
    "        print(f\"[skip] {csv_path} has no data rows.\")\n",
    "        return None\n",
    "\n",
    "    if df.empty:\n",
    "        print(f\"[skip] {csv_path} became empty after parsing.\")\n",
    "        return None\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # annotate with hyper-parameters\n",
    "    m = pat.fullmatch(csv_path.parent.name)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Folder name {csv_path.parent.name} does not match pattern\")\n",
    "    n_layers, direction, emb_dim, dropout = m.groups()\n",
    "\n",
    "    df[\"layers\"]  = int(n_layers)\n",
    "    df[\"bidir\"]   = (direction == \"bidir\")\n",
    "    df[\"emb_dim\"] = int(emb_dim)\n",
    "    df[\"dropout\"] = float(dropout)\n",
    "    # --------------------------------------------------------------\n",
    "    return df\n",
    "\n",
    "\n",
    "# build the list and drop the Nones\n",
    "frames = [f for f in (read_and_annotate(p) for p in root.rglob(\"*.csv\")) if f is not None]\n",
    "\n",
    "df_all = (\n",
    "    pd.concat(frames, ignore_index=True)\n",
    "      .set_index([\"layers\", \"bidir\", \"emb_dim\", \"dropout\", \"Epoch\"])\n",
    "      .sort_index()\n",
    ")\n",
    "\n",
    "# Build the big tidy DataFrame by concatenating all annotated DataFrames\n",
    "df_all = pd.concat([read_and_annotate(p) for p in csv_files], ignore_index=True)\n",
    "\n",
    "# (Optional) set a multi-index for easier slicing of hyper-parameters and sort it\n",
    "df_all = df_all.set_index([\"layers\", \"bidir\", \"emb_dim\", \"dropout\", \"Epoch\"]).sort_index()\n",
    "\n",
    "print(df_all.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4dbc1bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index to get columns back (if you set a multi-index before)\n",
    "df_all_reset = df_all.reset_index()\n",
    "\n",
    "# Select only the columns of interest: hyperparameters, Epoch, and validation/test metrics\n",
    "df_filtered = df_all_reset[[\n",
    "    \"layers\", \"bidir\", \"emb_dim\", \"dropout\", \"Epoch\",\"TrainLoss\",\"TrainAcc\",\n",
    "      \"TestLoss\", \"TestAcc\"\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b264c5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_options = [2,3,4,5]\n",
    "bidir_options = [True]  \n",
    "embedding_dims = [900,1800,2700,3600,4500]\n",
    "dropout_options = [0.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c845da6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " TrainAcc  TestAcc  TrainLoss  TestLoss  Epoch  layers  bidir  emb_dim  dropout\n",
      "   0.9763   0.4878     0.0672    4.0253     26       2   True     3600      0.7\n",
      "   0.8635   0.4808     0.3810    2.4368      7       4   True     1800      0.7\n",
      "   0.9916   0.4703     0.0274    4.8329    106       4   True     3600      0.7\n",
      "   0.9895   0.4701     0.0302    5.4919     85       2   True     2700      0.7\n",
      "   0.9906   0.4634     0.0268    4.8703     64       3   True     4500      0.7\n",
      "   0.9924   0.4569     0.0212    5.6277     86       2   True     4500      0.7\n",
      "   0.9895   0.4567     0.0301    5.5939     75       2   True     1800      0.7\n",
      "   0.5684   0.4566     1.0311    1.7083      3       4   True     2700      0.7\n",
      "   0.9419   0.4545     0.1689    3.2972     11       3   True     3600      0.7\n",
      "   0.9909   0.4524     0.0227    5.2683    105       3   True     2700      0.7\n",
      "   0.9152   0.4372     0.2550    2.9234      8       4   True     4500      0.7\n",
      "   0.9898   0.4368     0.0284    4.9810     47       3   True     1800      0.7\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 1. For every (layers, bidir, emb_dim, dropout) combo: row with max ValidAcc\n",
    "# ------------------------------------------------------------------\n",
    "best_valid_rows = (\n",
    "    df_filtered\n",
    "        .loc[                                  # keep the whole row …\n",
    "            df_filtered                         # … that has the index\n",
    "            .groupby([\"layers\", \"bidir\", \"emb_dim\", \"dropout\"])[\"TestAcc\"]\n",
    "            .idxmax()                           # of the epoch with max ValidAcc\n",
    "        ]\n",
    "        .sort_values(\"TestAcc\", ascending=False)   # 2. sort descending\n",
    "        .reset_index(drop=True)                     # tidy new index (optional)\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2. Show / export the ranking\n",
    "# ------------------------------------------------------------------\n",
    "cols_to_show = [\n",
    "    \"TrainAcc\", \"TestAcc\",          # main numbers of interest\n",
    "    \"TrainLoss\", \"TestLoss\",        # maybe useful too\n",
    "    \"Epoch\",                        # epoch where the max was reached\n",
    "    \"layers\", \"bidir\", \"emb_dim\", \"dropout\"  # hyper-parameters\n",
    "]\n",
    "\n",
    "print(best_valid_rows[cols_to_show].to_string(index=False))\n",
    "\n",
    "# If you prefer to save it:\n",
    "# best_valid_rows[cols_to_show].to_csv(\"best_by_valid_acc.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3084a4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " layers  bidir  emb_dim  dropout\n",
      "      2   True     3600      0.7\n",
      "      4   True     1800      0.7\n",
      "      4   True     3600      0.7\n",
      "      2   True     2700      0.7\n",
      "      3   True     4500      0.7\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Columns that define one hyper-parameter configuration\n",
    "# -------------------------------------------------------------\n",
    "param_cols = [\"layers\", \"bidir\", \"emb_dim\", \"dropout\"]\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 1) keep rows with TestAcc > 0.46\n",
    "# 2) keep only the hyper-parameter columns\n",
    "# 3) drop duplicate configurations\n",
    "# 4) reset the index (optional, just for neat printing / saving)\n",
    "# -------------------------------------------------------------\n",
    "good_param_sets = (\n",
    "    best_valid_rows                      # or df_filtered, if you prefer\n",
    "        .loc[best_valid_rows[\"TestAcc\"] > 0.46, param_cols]\n",
    "        .drop_duplicates()\n",
    "        .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(good_param_sets.to_string(index=False))\n",
    "\n",
    "# If you want to save it:\n",
    "# good_param_sets.to_csv(\"good_param_sets.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "490f4bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " MeanTestAcc  EpochCnt  FirstEpoch  LastEpoch  layers  bidir  emb_dim  dropout\n",
      "    0.439950        50          76        125       4   True     3600      0.7\n",
      "    0.434474        50          76        125       4   True     1800      0.7\n",
      "    0.426802        50          76        125       2   True     2700      0.7\n",
      "    0.425330        50          76        125       2   True     4500      0.7\n",
      "    0.422006        50          76        125       2   True     1800      0.7\n",
      "    0.421928        50          76        125       3   True     4500      0.7\n",
      "    0.421052        50          76        125       4   True     2700      0.7\n",
      "    0.420760        50          76        125       2   True     3600      0.7\n",
      "    0.414538        50          76        125       3   True     2700      0.7\n",
      "    0.404500        50          76        125       4   True     4500      0.7\n",
      "    0.403466        50          76        125       3   True     3600      0.7\n",
      "    0.401646        50          76        125       3   True     1800      0.7\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# 0. Hyper-parameter grid we are interested in\n",
    "# ---------------------------------------------------------------\n",
    "layer_options   = [2, 3, 4]\n",
    "bidir_options   = [True]\n",
    "embedding_dims  = [1800, 2700, 3600, 4500]\n",
    "dropout_options = [0.7]\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 1. Keep only the runs that belong to that grid\n",
    "# ---------------------------------------------------------------\n",
    "mask = (\n",
    "    df_filtered[\"layers\"].isin(layer_options)   &\n",
    "    df_filtered[\"bidir\"].isin(bidir_options)    &\n",
    "    df_filtered[\"emb_dim\"].isin(embedding_dims) &\n",
    "    df_filtered[\"dropout\"].isin(dropout_options)\n",
    ")\n",
    "df_sub = df_filtered.loc[mask]\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 2. Per run → retain the LAST 50 epochs (or fewer if run is short)\n",
    "# ---------------------------------------------------------------\n",
    "df_last50 = (\n",
    "    df_sub\n",
    "      .sort_values([\"layers\", \"bidir\", \"emb_dim\", \"dropout\", \"Epoch\"])\n",
    "      .groupby([\"layers\", \"bidir\", \"emb_dim\", \"dropout\"], as_index=False, group_keys=False)\n",
    "      .tail(50)                  # keep the 50 largest Epoch numbers of each group\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 3. Compute the mean TestAcc on those epochs\n",
    "# ---------------------------------------------------------------\n",
    "mean_test_acc = (\n",
    "    df_last50\n",
    "      .groupby([\"layers\", \"bidir\", \"emb_dim\", \"dropout\"], as_index=False)\n",
    "      .agg(MeanTestAcc=(\"TestAcc\", \"mean\"),\n",
    "           EpochCnt   =(\"Epoch\",   \"nunique\"),   # how many epochs actually used\n",
    "           FirstEpoch =(\"Epoch\",   \"min\"),       # first epoch kept (optional)\n",
    "           LastEpoch  =(\"Epoch\",   \"max\"))       # last  epoch kept (optional)\n",
    "      .sort_values(\"MeanTestAcc\", ascending=False)\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 4. Show / export\n",
    "# ---------------------------------------------------------------\n",
    "cols_to_show = [\"MeanTestAcc\", \"EpochCnt\", \"FirstEpoch\", \"LastEpoch\",\n",
    "                \"layers\", \"bidir\", \"emb_dim\", \"dropout\"]\n",
    "print(mean_test_acc[cols_to_show].to_string(index=False))\n",
    "\n",
    "# Optional: save to disk\n",
    "# mean_test_acc[cols_to_show].to_csv(\"mean_test_acc_last50.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a04728",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bd_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "148b689e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext import data, datasets\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define the LSTM-based model class\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, \n",
    "                 n_layers, bidirectional, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout if n_layers > 1 else 0.0)\n",
    "        # Linear layer input dimension depends on bidirectionality\n",
    "        fc_input_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        self.fc = nn.Linear(fc_input_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "    def forward(self, text, text_lengths):\n",
    "        # text: [sentence_length, batch_size]; text_lengths: [batch_size]\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        # Pack the sequence of embeddings\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'))\n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        # Unpack (pad) the sequence (output not used further here)\n",
    "        nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "        # hidden shape: [n_layers * num_directions, batch_size, hidden_dim]\n",
    "        if self.bidirectional:\n",
    "            # Concatenate the final forward and backward hidden states\n",
    "            hidden_combined = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        else:\n",
    "            # Use the final hidden state (from the last layer)\n",
    "            hidden_combined = hidden[-1,:,:]\n",
    "        # Apply dropout to final hidden state and pass through the linear layer\n",
    "        hidden_dropped = self.dropout(hidden_combined)\n",
    "        return self.fc(hidden_dropped)\n",
    "\n",
    "# Accuracy calculation\n",
    "def multiclass_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    preds: raw logits of shape [batch, n_classes]\n",
    "    y:     ground-truth indices, shape [batch]\n",
    "    \"\"\"\n",
    "    predicted_classes = preds.argmax(dim=1)\n",
    "    correct = (predicted_classes == y).float()\n",
    "    return correct.sum() / len(correct)\n",
    "\n",
    "# Training and evaluation functions\n",
    "def train_epoch(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        text, text_lengths = batch.text\n",
    "        predictions = model(text, text_lengths)\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        acc  = multiclass_accuracy(predictions, batch.label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate_epoch(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, text_lengths = batch.text\n",
    "            predictions = model(text, text_lengths)\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            acc = multiclass_accuracy(predictions, batch.label)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed = end_time - start_time\n",
    "    mins = int(elapsed // 60)\n",
    "    secs = int(elapsed % 60)\n",
    "    return mins, secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11496c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Choose a device once\n",
    "# ------------------------------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using:\", device)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Reload the torch-text Fields\n",
    "# ------------------------------------------------------------\n",
    "TEXT  = torch.load(\"TEXT_field.pth\",  weights_only=False)\n",
    "LABEL = torch.load(\"LABEL_field.pth\", weights_only=False)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Re-create the model\n",
    "# ------------------------------------------------------------\n",
    "INPUT_DIM     = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 4500\n",
    "HIDDEN_DIM    = 256\n",
    "OUTPUT_DIM    = len(LABEL.vocab)\n",
    "N_LAYERS      = 3\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT       = 0.7\n",
    "PAD_IDX       = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "print(PAD_IDX)\n",
    "print(\"============\")\n",
    "\n",
    "model = LSTMClassifier(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM,\n",
    "                       OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL,\n",
    "                       DROPOUT, pad_idx=PAD_IDX)\n",
    "\n",
    "# load weights, then move the whole model to the chosen device\n",
    "model.load_state_dict(torch.load(\"best_acc.pt\", map_location=device))\n",
    "model = model.to(device).eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f5a4a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3. Build your example and move it\n",
    "example_sentence = \"Great i love it \"\n",
    "\n",
    "tokens   = [tok.lower() for tok in TEXT.tokenize(example_sentence)]\n",
    "indices  = [TEXT.vocab.stoi[t] if t in TEXT.vocab.stoi\n",
    "            else TEXT.vocab.stoi[TEXT.unk_token] for t in tokens]\n",
    "\n",
    "text_tensor = torch.LongTensor(indices).unsqueeze(1).to(device)  # [seq_len, batch=1]\n",
    "text_length = torch.LongTensor([len(indices)]).to(device)        # [batch=1]\n",
    "\n",
    "# 4. Forward pass\n",
    "with torch.no_grad():\n",
    "    output = model(text_tensor, text_length)\n",
    "    prediction = output.argmax(dim=1).item()\n",
    "\n",
    "predicted_label = LABEL.vocab.itos[prediction]\n",
    "\n",
    "print(f\"Predicted label: {predicted_label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bd_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

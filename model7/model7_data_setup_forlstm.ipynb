{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9327633f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/21 16:03:50 WARN Utils: Your hostname, mustard7385-System-Product-Name, resolves to a loopback address: 127.0.1.1; using 192.168.0.110 instead (on interface eno1)\n",
      "25/11/21 16:03:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/21 16:03:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|              Phrase|Sentiment|\n",
      "+--------------------+---------+\n",
      "|wishin i could go...|        0|\n",
      "|@ verizon i'm hav...|        0|\n",
      "|please don't beli...|        0|\n",
      "|please sort out a...|        0|\n",
      "|feature fix the e...|        0|\n",
      "|disrespectful. an...|        0|\n",
      "|i think the game ...|        0|\n",
      "|fuck hell, you th...|        0|\n",
      "|@ jukinmedia yout...|        0|\n",
      "|omgggg guy very b...|        0|\n",
      "+--------------------+---------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|              Phrase|Sentiment|\n",
      "+--------------------+---------+\n",
      "|cold war black op...|        0|\n",
      "|so add a fucking ...|        0|\n",
      "|this be the bad @...|        0|\n",
      "|'s liberal regres...|        0|\n",
      "|so when i try to ...|        0|\n",
      "|my first run a an...|        0|\n",
      "|i'm still not buy...|        0|\n",
      "|fuck verizon. the...|        0|\n",
      "|news: pubg mobile...|        0|\n",
      "|4 hey rhandlerr r...|        0|\n",
      "+--------------------+---------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, regexp_replace, lower, trim, udf\n",
    "from pyspark.sql.types import *\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Additional imports for augmentation\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def lemmatize(data_str):\n",
    "    \"\"\"Lemmatize text using NLTK WordNetLemmatizer with POS tagging.\"\"\"\n",
    "    if not data_str or not isinstance(data_str, str):\n",
    "        return ''\n",
    "    try:\n",
    "        lmtzr = WordNetLemmatizer()\n",
    "        words = data_str.split()\n",
    "        if not words:\n",
    "            return ''\n",
    "        tagged_words = nltk.pos_tag(words)\n",
    "        lemmatized_words = []\n",
    "        for word, tag in tagged_words:\n",
    "            if tag.startswith('V'):\n",
    "                lemma = lmtzr.lemmatize(word, pos='v')\n",
    "            elif tag.startswith('J'):\n",
    "                lemma = lmtzr.lemmatize(word, pos='a')\n",
    "            elif tag.startswith('R'):\n",
    "                lemma = lmtzr.lemmatize(word, pos='r')\n",
    "            else:\n",
    "                lemma = lmtzr.lemmatize(word, pos='n')\n",
    "            lemmatized_words.append(lemma)\n",
    "        return ' '.join(lemmatized_words)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Lemmatization failed: {e}\")\n",
    "        return data_str\n",
    "\n",
    "def clean_tweets(df, text_column=\"Phrase\"):\n",
    "    \"\"\"Clean Twitter text data for sentiment analysis.\"\"\"\n",
    "    cleaned_df = df.withColumn(text_column, col(text_column).cast(StringType()))\n",
    "    # Remove URLs\n",
    "    cleaned_df = cleaned_df.withColumn(\n",
    "        text_column, regexp_replace(col(text_column), r'http\\S+|www\\.\\S+|https?://\\S+|t\\.co/\\S+', '')\n",
    "    )\n",
    "    # Remove user mentions\n",
    "    cleaned_df = cleaned_df.withColumn(\n",
    "        text_column, regexp_replace(col(text_column), r'@\\w+', '')\n",
    "    )\n",
    "    # Remove HTML tags/entities (e.g., <unk>)\n",
    "    cleaned_df = cleaned_df.withColumn(\n",
    "        text_column, regexp_replace(col(text_column), r'<[^>]+>', '')\n",
    "    )\n",
    "    # Remove special unicode sequences\n",
    "    cleaned_df = cleaned_df.withColumn(\n",
    "        text_column, regexp_replace(col(text_column), r'â\\S+|â\\S+|â\\S+', '')\n",
    "    )\n",
    "    # Normalize whitespace and newlines\n",
    "    cleaned_df = cleaned_df.withColumn(\n",
    "        text_column, regexp_replace(col(text_column), r'\\s+', ' ')\n",
    "    )\n",
    "    # Lowercase and trim\n",
    "    cleaned_df = cleaned_df.withColumn(text_column, lower(col(text_column)))\n",
    "    cleaned_df = cleaned_df.withColumn(text_column, trim(col(text_column)))\n",
    "    # Remove empty or null rows\n",
    "    cleaned_df = cleaned_df.filter(col(text_column) != '').filter(col(text_column).isNotNull())\n",
    "    # Lemmatize text\n",
    "    lem_udf = udf(lemmatize, StringType())\n",
    "    cleaned_df = cleaned_df.withColumn(text_column, lem_udf(col(text_column)))\n",
    "    return cleaned_df\n",
    "\n",
    "def load_and_preprocess_data(file_path, text_column=\"Phrase\", sentiment_column=\"Sentiment\", augment=False):\n",
    "    \"\"\"\n",
    "    Load CSV data, clean it, and optionally augment it for sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    ss = SparkSession.builder.appName(\"TwitterSentimentAnalysis\").getOrCreate()\n",
    "    df = ss.read.csv(file_path, header=True, inferSchema=True, quote='\"', escape='\"')\n",
    "    cleaned_df = clean_tweets(df, text_column)\n",
    "    cleaned_df = cleaned_df.withColumn(sentiment_column, col(sentiment_column).cast(IntegerType()))\n",
    " \n",
    "    return cleaned_df\n",
    "\n",
    "# Example usage:\n",
    "train_df = load_and_preprocess_data(\"../Twitter_data/traindata7.csv\", augment=True) \n",
    "test_df  = load_and_preprocess_data(\"../Twitter_data/testdata7.csv\", augment=False)\n",
    "train_df.show(10)\n",
    "test_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d4410dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set rows : 596\n",
      "Test set rows     : 397\n"
     ]
    }
   ],
   "source": [
    "# Total rows in each DataFrame\n",
    "n_train = train_df.count()\n",
    "n_test  = test_df.count()\n",
    "\n",
    "print(f\"Training set rows : {n_train:,}\")\n",
    "print(f\"Test set rows     : {n_test:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce6832b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 1. Collapse to a sensible number of partitions (optional, but reduces shuffle)\n",
    "train_small = train_df.coalesce(1)\n",
    "test_small  = test_df.coalesce(1)\n",
    "\n",
    "# 2. Collect to the driver as Pandas\n",
    "train_pd = train_small.toPandas()\n",
    "test_pd  = test_small.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "583db7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# helper\n",
    "# ------------------------------------------------------------------ #\n",
    "def export_split(\n",
    "    df: pd.DataFrame,\n",
    "    split: str,                 # \"train\" or \"test\"\n",
    "    base_dir: str | Path = \"./data\",\n",
    "    text_col: str = \"Phrase\",\n",
    "    label_col: str = \"Sentiment\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Write a Pandas DataFrame to:\n",
    "      • <base_dir>/<split>.csv      (header: Phrase,Sentiment)\n",
    "      • <base_dir>/<split>/<label>/<row_id>.txt\n",
    "    \"\"\"\n",
    "    base_dir = Path(base_dir).resolve()\n",
    "    base_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    csv_path = base_dir / f\"{split}.csv\"\n",
    "    df[[text_col, label_col]].to_csv(csv_path, index=False)\n",
    "\n",
    "    # 2️individual .txt files ----------------------------------------------\n",
    "    for row_id, (phrase, label) in df[[text_col, label_col]].iterrows():\n",
    "        tgt_dir  = base_dir / split / str(label)\n",
    "        tgt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # use the DataFrame’s integer index as <row_id>; change if you prefer\n",
    "        txt_path = tgt_dir / f\"{row_id}.txt\"\n",
    "        txt_path.write_text(str(phrase), encoding=\"utf-8\")\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# call the helper for train / test\n",
    "# ------------------------------------------------------------------ #\n",
    "export_split(train_pd, \"train\")   # writes train.csv and train/<sent>/…\n",
    "export_split(test_pd,  \"test\")    # writes test.csv  and test/<sent>/…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a04728",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bd_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd9ab1a2",
   "metadata": {},
   "source": [
    "# Story board\n",
    "\n",
    "### 1. Data Exploration (yapping + some visualization about dataset) --> Haige\n",
    "### 2. Initialize the baseline model + explain about how to improve it (\"step 1\" of instruction 3) --> Johnny\n",
    "### 3. Initialize the model and explain about the development of each model one by one. (Here you also discuss the \"step 2\" of instruction 3) --> everyone\n",
    "### 4. Compare the best version of everyone's model performance. --> Jason"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f843aa66",
   "metadata": {},
   "source": [
    "## Section 1: Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cceb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63b5d93",
   "metadata": {},
   "source": [
    "## Section 2: Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a7aeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3a5147",
   "metadata": {},
   "source": [
    "## Sec 3 Model1: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b763c22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bef124e",
   "metadata": {},
   "source": [
    "## Sec 3 Model2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecf83802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_dir exists: True\n",
      "config.json    : True\n",
      "fields.pth     : True\n",
      "label_field.pth: True\n",
      "model state    : True\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "run_dir = \"model2/saved_models\"\n",
    "print(\"run_dir exists:\", os.path.isdir(run_dir))\n",
    "print(\"config.json    :\", os.path.isfile(os.path.join(run_dir, \"config.json\")))\n",
    "print(\"fields.pth     :\", os.path.isfile(os.path.join(run_dir, \"fields.pth\")))\n",
    "print(\"label_field.pth:\", os.path.isfile(os.path.join(run_dir, \"label_field.pth\")))\n",
    "print(\"model state    :\", bool(glob.glob(os.path.join(run_dir, \"best_*.pt\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b1de76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0]\n",
      "Loading datasets...\n",
      "Building vocabulary...\n",
      "Starting training...\n",
      "Epoch 1 | Train loss=1.1290 acc=0.5188 | Test loss=0.8591 acc=0.6511\n",
      "Epoch 2 | Train loss=0.9126 acc=0.6284 | Test loss=0.6550 acc=0.7267\n",
      "Epoch 3 | Train loss=0.7672 acc=0.6974 | Test loss=0.4709 acc=0.8177\n",
      "{'best_test_acc': 0.8176775162036602, 'best_test_loss': 0.4708516586285371, 'data': {'fields': 'model2/saved_models/fields.pth', 'label_field': 'model2/saved_models/label_field.pth', 'best_acc': 'model2/saved_models/best_acc.pt', 'best_loss': 'model2/saved_models/best_loss.pt'}, 'config': {'embedding_dim': 300, 'hidden_dim': 256, 'n_layers': 2, 'bidirectional': True, 'dropout': 0.5, 'pad_idx': 1, 'vocab_size': 33270, 'output_dim': 4}}\n",
      "[1, 0]\n",
      "[1, 0]\n"
     ]
    }
   ],
   "source": [
    "from model2.lstm_package import train_lstm, load_lstm, predict_sentiment\n",
    "from pathlib import Path\n",
    "\n",
    "run_dir = Path(\"./model2/saved_best\")\n",
    "run_dir.mkdir(parents=True, exist_ok=True)\n",
    "run_dir  = \"./model2/saved_best\"\n",
    "\n",
    "data_dir = Path(\"./model2/data\")\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "data_dir = \"./model2/data\"                 # data/train / data/test\n",
    "\n",
    "\n",
    "# load\n",
    "loaded   = load_lstm(run_dir)\n",
    "model    = loaded['model']\n",
    "TEXT     = loaded['TEXT']\n",
    "\n",
    "# inference\n",
    "texts = [\"I love this product\", \"This is awful\"]\n",
    "print(predict_sentiment(model, TEXT, texts))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from model2.lstm_package import train_lstm, load_lstm, predict_sentiment\n",
    "from pathlib import Path\n",
    "\n",
    "run_dir = Path(\"model2/saved_models\")\n",
    "run_dir.mkdir(parents=True, exist_ok=True)\n",
    "run_dir  = \"model2/saved_models\"\n",
    "\n",
    "data_dir = Path(\"model2/data\")\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "data_dir = \"model2/data\"                 # data/train / data/test\n",
    "\n",
    "# train\n",
    "res = train_lstm(data_dir, run_dir, n_epochs=3, batch_size=32)\n",
    "print(res)\n",
    "\n",
    "# load\n",
    "loaded   = load_lstm(run_dir)\n",
    "model    = loaded['model']\n",
    "TEXT     = loaded['TEXT']\n",
    "\n",
    "# inference\n",
    "texts = [\"I love this product\", \"This is awful\"]\n",
    "print(predict_sentiment(model, TEXT, texts))\n",
    "\n",
    "\n",
    "\n",
    "# load\n",
    "loaded   = load_lstm(run_dir)\n",
    "model    = loaded['model']\n",
    "TEXT     = loaded['TEXT']\n",
    "\n",
    "# inference\n",
    "texts = [\"I love this product\", \"This is awful\"]\n",
    "print(predict_sentiment(model, TEXT, texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a02cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a57aac",
   "metadata": {},
   "source": [
    "# Sec 3 Model3: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2416a8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6758189",
   "metadata": {},
   "source": [
    "## Sec 3 Model4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c4d387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7297633f",
   "metadata": {},
   "source": [
    "## Sec 3 Model5: \n",
    "#### Text Enbedding Model: TF-IDF\n",
    "#### Data Analytics Model: Naive Bayes\n",
    "#### Python Libaraies: Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaf9a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement here\n",
    "# Initialize the model and explain about the development of each model one by one. (Here you also discuss the \"step 2\" of instruction 3)\n",
    "\n",
    "'''\n",
    "1. Preparation of libraries, dataset paths, and sample texts.\n",
    "'''\n",
    "from model5.package.model import load_saved_model, evaluate_saved_model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "pretrain_csv_path = 'Twitter_data/pre_traindata7.csv'\n",
    "train_csv_path = 'Twitter_data/traindata7.csv'\n",
    "test_csv_path = 'Twitter_data/testdata7.csv'\n",
    "\n",
    "sample_texts = [\"I love this!\", \"I hate this!\", \"Sounds alright.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f3a345",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "2. Test with standard model trained with standard dataset.\n",
    "'''\n",
    "# Initialize the models\n",
    "model5_standard = load_saved_model('model5/package/saved_models')\n",
    "\n",
    "model5_standard_tfidf_model: TfidfVectorizer = model5_standard['embedding']\n",
    "model5_standard_nb_model: MultinomialNB = model5_standard['model']\n",
    "\n",
    "# Evaluate the model on training and testing datasets\n",
    "model5_standard_eval_result = evaluate_saved_model(model5_standard, train_csv_path, test_csv_path)\n",
    "\n",
    "model5_standard_train_loss      = model5_standard_eval_result.get('train').get('loss')\n",
    "model5_standard_train_accuracy  = model5_standard_eval_result.get('train').get('accuracy')\n",
    "model5_standard_test_loss       = model5_standard_eval_result.get('test').get('loss')\n",
    "model5_standard_test_accuracy   = model5_standard_eval_result.get('test').get('accuracy')\n",
    "\n",
    "print(\"Evaluation Results for Standard Model:\")\n",
    "print(f\"Train loss: {model5_standard_train_loss}, Train accuracy: {model5_standard_train_accuracy}\")\n",
    "print(f\"Test loss: {model5_standard_test_loss}, Test accuracy: {model5_standard_test_accuracy}\")\n",
    "\n",
    "# Prediction Result\n",
    "embedded_texts = model5_standard_tfidf_model.transform(sample_texts)\n",
    "nb_predictions = model5_standard_nb_model.predict(embedded_texts)\n",
    "\n",
    "print(\"Sample Predictions:\")\n",
    "for t, p in zip(sample_texts, nb_predictions):\n",
    "    print(f\"Text: '{t}' => Predicted Sentiment: {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaeb8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "3. Test with standard model trained with pre-trained dataset.\n",
    "'''\n",
    "# Initialize the models\n",
    "model5_pretrain = load_saved_model('model5/package/saved_models_pretrain7')\n",
    "\n",
    "model5_pretrain_tfidf_model: TfidfVectorizer = model5_pretrain['embedding']\n",
    "model5_pretrain_nb_model: MultinomialNB = model5_pretrain['model']\n",
    "\n",
    "# Evaluate the model on training and testing datasets\n",
    "model5_pretrain_eval_result = evaluate_saved_model(model5_pretrain, pretrain_csv_path, test_csv_path)\n",
    "\n",
    "model5_pretrain_train_loss      = model5_pretrain_eval_result.get('train').get('loss')\n",
    "model5_pretrain_train_accuracy  = model5_pretrain_eval_result.get('train').get('accuracy')\n",
    "model5_pretrain_test_loss       = model5_pretrain_eval_result.get('test').get('loss')\n",
    "model5_pretrain_test_accuracy   = model5_pretrain_eval_result.get('test').get('accuracy')\n",
    "\n",
    "print(\"Evaluation Results for Pretrained Model:\")\n",
    "print(f\"Train loss: {model5_pretrain_train_loss}, Train accuracy: {model5_pretrain_train_accuracy}\")\n",
    "print(f\"Test loss: {model5_pretrain_test_loss}, Test accuracy: {model5_pretrain_test_accuracy}\")\n",
    "\n",
    "# Prediction Result\n",
    "embedded_texts = model5_pretrain_tfidf_model.transform(sample_texts)\n",
    "nb_predictions = model5_pretrain_nb_model.predict(embedded_texts)\n",
    "\n",
    "print(\"Sample Predictions:\")\n",
    "for t, p in zip(sample_texts, nb_predictions):\n",
    "    print(f\"Text: '{t}' => Predicted Sentiment: {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c777debd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "313fed82",
   "metadata": {},
   "source": [
    "#### Explanation later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5715d77c",
   "metadata": {},
   "source": [
    "## Sec 3 Model6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9a287c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e5b18b",
   "metadata": {},
   "source": [
    "## Section 4: Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0e84b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement here\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bd_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
